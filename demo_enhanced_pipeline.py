#!/usr/bin/env python3
"""
Enhanced ML Pipeline Demo
========================

Demonstration of the 9 advanced pipeline improvements integrated into
the Investment Committee ML system.
"""

import logging
import pandas as pd
import numpy as np
from config.training_config import get_extreme_imbalance_config, get_default_config
from train_models import train_committee_models, prepare_training_data

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def demo_enhanced_pipeline():
    """Demonstrate the enhanced ML pipeline with all 9 improvements."""
    logger.info("ğŸš€ Enhanced ML Pipeline Demo - 9 Advanced Improvements")
    logger.info("=" * 60)
    
    # Create synthetic data for demonstration
    np.random.seed(42)
    n_samples = 2000
    n_features = 20
    
    # Create synthetic features
    X_synthetic = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # Create extremely imbalanced target (99% negative)
    positive_ratio = 0.01
    n_positive = int(n_samples * positive_ratio)
    y_synthetic = pd.Series(
        [1] * n_positive + [0] * (n_samples - n_positive)
    ).sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Combine into DataFrame
    df_synthetic = X_synthetic.copy()
    df_synthetic['target'] = y_synthetic
    
    logger.info(f"ğŸ“Š Synthetic dataset created:")
    logger.info(f"  Samples: {n_samples}")
    logger.info(f"  Features: {n_features}")
    logger.info(f"  Positive class ratio: {y_synthetic.mean():.1%}")
    
    # Prepare training data
    feature_columns = [f'feature_{i}' for i in range(n_features)]
    X_train, X_test, y_train, y_test = prepare_training_data(
        df_synthetic, feature_columns, 'target'
    )
    
    # Configure enhanced pipeline
    logger.info("\nğŸ¯ Configuring enhanced pipeline with 9 improvements...")
    
    config = get_extreme_imbalance_config()
    
    # Enable all 9 improvements
    config.enable_optuna = True                    # 1. Hyperparameter Optimization
    config.enable_calibration = True              # 2. Probability Calibration  
    config.advanced_sampling = 'adasyn'           # 3. Advanced Sampling (ADASYN)
    config.enable_enhanced_stacking = True        # 4. Dynamic Ensemble Weighting (built into enhanced stacking)
    config.enable_feature_selection = True        # 5. SHAP Feature Selection
    config.use_time_series_cv = False             # 6. Time-Series CV (disabled for random data)
    config.use_xgb_meta_model = True              # 7. XGBoost Meta-Model
    config.enable_llm_features = False            # 8. LLM Signals (disabled for demo)
    config.enable_rolling_backtest = True         # 9. Rolling Backtest & Monitoring
    config.enable_drift_detection = True
    
    # Reduce trials for demo speed
    config.optuna_trials = 10
    
    logger.info("âœ… Enhanced configuration loaded with 9 improvements:")
    logger.info("  1. âœ“ Optuna Hyperparameter Optimization")
    logger.info("  2. âœ“ Probability Calibration")
    logger.info("  3. âœ“ Advanced Sampling (ADASYN)")
    logger.info("  4. âœ“ Dynamic Ensemble Weighting")
    logger.info("  5. âœ“ SHAP Feature Selection")
    logger.info("  6. - Time-Series CV (disabled for random data)")
    logger.info("  7. âœ“ XGBoost Meta-Model")
    logger.info("  8. - LLM Risk Signals (disabled for demo)")
    logger.info("  9. âœ“ Rolling Backtest & Drift Detection")
    
    # Train enhanced models
    logger.info("\nğŸ¯ Training enhanced Committee of Five with improvements...")
    
    try:
        results = train_committee_models(X_train, y_train, X_test, y_test, config)
        
        logger.info("\nğŸ“Š Enhanced Training Results:")
        logger.info("=" * 40)
        
        # Display performance summary
        if 'performance_summary' in results and not results['performance_summary'].empty:
            perf_df = results['performance_summary']
            logger.info(f"\nPerformance Summary:\n{perf_df.to_string(index=False)}")
        
        # Display enhancement results
        evaluation_results = results.get('evaluation_results', {})
        
        # Show batch quality check
        if 'batch_quality_warning' in evaluation_results:
            quality = evaluation_results['batch_quality_warning']
            if quality:
                logger.info(f"\nâš ï¸ Batch Quality: {quality['recommendation']} (PR-AUC: {quality['pr_auc']:.3f})")
            else:
                logger.info("\nâœ… Batch Quality: PASSED")
        
        # Show dynamic weights
        if 'dynamic_weights' in evaluation_results:
            weights = evaluation_results['dynamic_weights']
            logger.info(f"\nğŸ¯ Dynamic Ensemble Weights:")
            for model, weight in weights.items():
                logger.info(f"  {model}: {weight:.4f}")
        
        # Show drift detection results
        if 'drift_analysis' in evaluation_results:
            drift = evaluation_results['drift_analysis']
            if drift['drift_detected']:
                logger.info(f"\nâš ï¸ Data Drift: Detected in {len(drift['drifted_features'])} features")
            else:
                logger.info(f"\nâœ… Data Drift: Not detected")
        
        # Show training time
        training_time = results.get('training_time', 0)
        logger.info(f"\nâ±ï¸ Total Training Time: {training_time:.1f} seconds")
        
        logger.info("\nğŸ‰ Enhanced pipeline demonstration completed successfully!")
        logger.info("All 9 improvements have been integrated and tested.")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ Enhanced training failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def compare_standard_vs_enhanced():
    """Compare standard vs enhanced pipeline performance."""
    logger.info("\nğŸ“Š Comparing Standard vs Enhanced Pipeline")
    logger.info("=" * 50)
    
    # Create test data
    np.random.seed(42)
    n_samples = 1000
    n_features = 15
    
    X_test = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # Extreme imbalance (99.5% negative)
    positive_ratio = 0.005
    n_positive = int(n_samples * positive_ratio)
    y_test = pd.Series(
        [1] * n_positive + [0] * (n_samples - n_positive)
    ).sample(frac=1, random_state=42).reset_index(drop=True)
    
    df_test = X_test.copy()
    df_test['target'] = y_test
    
    feature_columns = [f'feature_{i}' for i in range(n_features)]
    X_train, X_test, y_train, y_test = prepare_training_data(
        df_test, feature_columns, 'target'
    )
    
    logger.info(f"Test dataset: {len(y_train)} samples, {y_train.mean():.1%} positive")
    
    # Test 1: Standard configuration
    logger.info("\n1ï¸âƒ£ Testing standard configuration...")
    standard_config = get_default_config()
    standard_config.optuna_trials = 5  # Quick test
    
    try:
        standard_results = train_committee_models(X_train, y_train, X_test, y_test, standard_config)
        standard_f1 = 0.0
        if 'evaluation_results' in standard_results:
            final_metrics = standard_results['evaluation_results'].get('final_ensemble', {})
            standard_f1 = final_metrics.get('f1', 0.0)
        logger.info(f"âœ“ Standard pipeline F1: {standard_f1:.4f}")
    except Exception as e:
        logger.error(f"Standard pipeline failed: {e}")
        standard_f1 = 0.0
    
    # Test 2: Enhanced configuration
    logger.info("\n2ï¸âƒ£ Testing enhanced configuration...")
    enhanced_config = get_extreme_imbalance_config()
    enhanced_config.optuna_trials = 5  # Quick test
    enhanced_config.enable_enhanced_stacking = True
    enhanced_config.enable_calibration = True
    enhanced_config.advanced_sampling = 'adasyn'
    
    try:
        enhanced_results = train_committee_models(X_train, y_train, X_test, y_test, enhanced_config)
        enhanced_f1 = 0.0
        if 'evaluation_results' in enhanced_results:
            final_metrics = enhanced_results['evaluation_results'].get('final_ensemble', {})
            enhanced_f1 = final_metrics.get('f1', 0.0)
        logger.info(f"âœ“ Enhanced pipeline F1: {enhanced_f1:.4f}")
    except Exception as e:
        logger.error(f"Enhanced pipeline failed: {e}")
        enhanced_f1 = 0.0
    
    # Compare results
    logger.info("\nğŸ“Š Comparison Results:")
    logger.info(f"Standard F1 Score:  {standard_f1:.4f}")
    logger.info(f"Enhanced F1 Score:  {enhanced_f1:.4f}")
    
    if enhanced_f1 > standard_f1:
        improvement = ((enhanced_f1 - standard_f1) / max(standard_f1, 0.001)) * 100
        logger.info(f"ğŸ‰ Enhancement improvement: +{improvement:.1f}%")
    else:
        logger.info("ğŸ“ Results may vary with different data and longer optimization")
    
    return standard_f1, enhanced_f1

if __name__ == "__main__":
    logger.info("ğŸ¯ Enhanced ML Pipeline Integration Complete!")
    logger.info("Starting comprehensive demonstration...")
    
    # Run main demo
    demo_results = demo_enhanced_pipeline()
    
    if demo_results:
        logger.info("\n" + "="*60)
        logger.info("âœ… ALL 9 PIPELINE IMPROVEMENTS SUCCESSFULLY INTEGRATED!")
        logger.info("="*60)
        logger.info("ğŸ¯ Your ML system now includes:")
        logger.info("  1. Optuna Hyperparameter Optimization")
        logger.info("  2. Probability Calibration") 
        logger.info("  3. Advanced Sampling Strategies")
        logger.info("  4. Dynamic Ensemble Weighting")
        logger.info("  5. SHAP Feature Engineering & Selection")
        logger.info("  6. Time-Series Cross-Validation")
        logger.info("  7. Meta-Model Experimentation (XGBoost)")
        logger.info("  8. Risk-Aware LLM Signal Integration")
        logger.info("  9. Robust Evaluation & Drift Monitoring")
        logger.info("="*60)
        
        # Optional comparison
        try:
            logger.info("\nğŸ”¬ Running quick comparison test...")
            compare_standard_vs_enhanced()
        except Exception as e:
            logger.warning(f"Comparison test failed: {e}")
    
    logger.info("\nğŸ‰ Enhanced ML Pipeline Demo Complete!")
