#!/usr/bin/env python3
"""
Committee of Five Training Script
=================================

This script implements a "Committee of Five" ensemble for the Investment
Committee project.  It trains five different base modelsâ€”XGBoost,
LightGBM, CatBoost, Random Forest, and Support Vector Classificationâ€”on
the same training data, using the same feature set and train/test
splits.  Each model outputs probabilities on the validation/test set.
Those probabilities are stacked as new features and used to train a
logistic regression metaâ€‘model, which produces the final ensemble
predictions.  Metrics (accuracy, F1 and PRâ€‘AUC) are computed for each
base model and for the stacked ensemble.  Confusion matrices and
bar charts of the metrics are saved to disk for diagnostics.  A
training summary is logged to CSV via the ``utils.training_logger``
module.

To keep the file selfâ€‘contained, light utility functions for
balancing datasets and preparing train/test splits are included.
This script does **not** rely on the previous neural network or
neural_predictor componentsâ€”those references have been removed.

Usage
-----
This file can be used as a module within a larger training pipeline.
To run it as a script, you must first assemble a DataFrame with
engineered features and a target column named ``target``.  Then
invoke ``main()`` with appropriate arguments or adapt the call to
your application.
"""

import argparse
import logging
import os
import time
from typing import Dict, Any, Tuple, List

import numpy as np
import pandas as pd

# Optional imports for visualization
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    VISUALIZATION_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    VISUALIZATION_AVAILABLE = False

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GroupShuffleSplit, train_test_split

# Oversampling is optional; handle missing imblearn gracefully
try:
    from imblearn.over_sampling import RandomOverSampler
    IMBLEARN_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    IMBLEARN_AVAILABLE = False

from utils.helpers import compute_classification_metrics, compute_classification_metrics_with_threshold, find_optimal_threshold
from utils.training_logger import log_training_summary

# Import Alpaca data collection
from data_collection_alpaca import AlpacaDataCollector

# =============================================================================
# CONFIGURATION CONSTANTS
# =============================================================================

# Data balancing parameters
DEFAULT_MAX_RATIO = 2.5                    # Maximum majority:minority ratio before capping
DEFAULT_DESIRED_RATIO = 0.6                # Target ratio for controlled balancing (60% majority, 40% minority)
DEFAULT_MINORITY_THRESHOLD = 100           # Threshold below which to use oversampling

# Out-of-fold stacking parameters
DEFAULT_N_FOLDS = 5                        # Number of cross-validation folds
DEFAULT_RANDOM_STATE = 42                  # Random state for reproducibility
DEFAULT_SHUFFLE = True                     # Whether to shuffle in StratifiedKFold

# Calibration parameters
DEFAULT_CALIBRATION_METHOD = 'isotonic'    # 'isotonic' or 'sigmoid'
DEFAULT_CALIBRATION_CV = 3                 # Number of CV folds for calibration

# SMOTE/SMOTEENN parameters
DEFAULT_SMOTE_K_NEIGHBORS = 5              # k_neighbors for SMOTE (will be adapted for small datasets)

# Meta-model parameters
DEFAULT_META_MODEL = 'logistic_regression' # Meta-model type
DEFAULT_META_MAX_ITER = 1000               # Max iterations for LogisticRegression

# Visualization parameters
DEFAULT_CHART_FIGURE_WIDTH = 10           # Width for metric comparison charts
DEFAULT_CHART_FIGURE_HEIGHT = 6           # Height for metric comparison charts  
DEFAULT_MATRIX_FIGURE_WIDTH = 8           # Width for confusion matrix plots
DEFAULT_MATRIX_FIGURE_HEIGHT = 6          # Height for confusion matrix plots
DEFAULT_CHART_DPI = 150                   # DPI for saved charts

# Timing and logging
ENABLE_TIMING = True                       # Whether to measure and log timing information

# Aliases for easier access
CHART_FIGURE_WIDTH = DEFAULT_CHART_FIGURE_WIDTH
CHART_FIGURE_HEIGHT = DEFAULT_CHART_FIGURE_HEIGHT
MATRIX_FIGURE_WIDTH = DEFAULT_MATRIX_FIGURE_WIDTH
MATRIX_FIGURE_HEIGHT = DEFAULT_MATRIX_FIGURE_HEIGHT
CHART_DPI = DEFAULT_CHART_DPI

# =============================================================================

# Timing and logging utility
def log_timing(operation_name: str, start_time: float = None, enable: bool = None) -> None:
    """Log the duration of an operation if timing is enabled."""
    # Use global ENABLE_TIMING if enable parameter not provided
    timing_enabled = enable if enable is not None else ENABLE_TIMING
    
    if timing_enabled:
        if start_time is not None:
            duration = time.time() - start_time
            logger.info(f"  âœ”ï¸ {operation_name} completed in {duration:.1f}s")
        else:
            # Simple log message without timing
            logger.info(f"  âœ”ï¸ {operation_name}")

# Ensure log directory exists before configuring logging
os.makedirs('logs', exist_ok=True)
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Advanced ML imports for class imbalance (after logger is defined)
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.combine import SMOTEENN
    ADVANCED_SAMPLING_AVAILABLE = True
except ImportError:
    ADVANCED_SAMPLING_AVAILABLE = False
    logger.warning("Advanced sampling (SMOTE) not available. Install with: pip install imbalanced-learn")

try:
    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.model_selection import StratifiedKFold
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score
    CALIBRATION_AVAILABLE = True
except ImportError:
    CALIBRATION_AVAILABLE = False
    logger.warning("Calibration not available in this sklearn version")

# Import model classes
from models.xgboost_model import XGBoostModel
from models.lightgbm_model import LightGBMModel
from models.catboost_model import CatBoostModel
from models.random_forest_model import RandomForestModel
from models.svc_model import SVMClassifier


def cap_majority_ratio(X: pd.DataFrame, y: pd.Series, max_ratio: float = 2.5) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Cap extreme majorityâ€“minority ratios to prevent training instability.

    Args:
        X: Feature matrix (DataFrame)
        y: Target labels (Series)
        max_ratio: Maximum allowed majority:minority ratio

    Returns:
        A tuple ``(X_capped, y_capped)`` with capped ratios.  If the
        existing ratio is below ``max_ratio``, the original data is returned.
    """
    # Convert to DataFrame for easier manipulation if necessary
    if not isinstance(X, pd.DataFrame):
        X = pd.DataFrame(X, columns=[f"f{i}" for i in range(X.shape[1])])
    df = X.copy()
    df['y'] = y.values if hasattr(y, 'values') else np.array(y)

    n0 = (df.y == 0).sum()
    n1 = (df.y == 1).sum()
    if min(n0, n1) == 0:
        return X, y
    current_ratio = max(n0, n1) / min(n0, n1)
    if current_ratio <= max_ratio:
        return X, y
    # Identify majority and minority classes
    maj, mino = (0, 1) if n0 > n1 else (1, 0)
    keep_majority = int(max_ratio * min(n0, n1))
    df_major = df[df.y == maj].sample(keep_majority, random_state=42)
    df_mino = df[df.y == mino]
    df_new = pd.concat([df_major, df_mino]).sample(frac=1, random_state=42)
    return df_new.drop('y', axis=1), df_new['y'].values


def prepare_balanced_data(X_train: pd.DataFrame, y_train: pd.Series, method: str = 'smote') -> Tuple[pd.DataFrame, pd.Series]:
    """
    Advanced data balancing using multiple techniques for extreme class imbalance.
    
    Args:
        X_train: Training features
        y_train: Training labels  
        method: Balancing method ('smote', 'smoteenn', 'combined', 'basic')
        
    Returns:
        Balanced training data
    """
    t0 = time.time()
    logger.info(f"Preparing balanced data using method: {method}")
    
    # Check if we have multiple classes
    unique_classes = y_train.unique()
    if len(unique_classes) < 2:
        logger.warning(f"Only one class found in training data: {unique_classes}. Returning original data.")
        return X_train.copy(), y_train.copy()
    
    # For basic balancing, delegate to balance_dataset
    if method == 'basic' or not ADVANCED_SAMPLING_AVAILABLE:
        if method != 'basic':
            logger.warning(f"Advanced sampling not available, falling back to basic balancing")
        result = balance_dataset(X_train, y_train)
        log_timing(f"Basic balancing", t0)
        return result
    
    # Log original distribution
    original_counts = y_train.value_counts().sort_index()
    logger.info(f"Original class distribution: {original_counts.to_dict()}")
    
    # Check if we have enough samples for advanced methods
    min_class_count = min(original_counts)
    if min_class_count < 2:
        logger.warning(f"Insufficient samples for balancing (min class: {min_class_count}). Using basic method.")
        result = balance_dataset(X_train, y_train)
        log_timing(f"Fallback basic balancing", t0)
        return result
    
    try:
        if method == 'smote':
            # SMOTE with adaptive k-neighbors for small datasets
            k_neighbors = min(DEFAULT_SMOTE_K_NEIGHBORS, min_class_count - 1) if min_class_count > 1 else 1
            
            if min_class_count < 2:
                logger.warning("Not enough minority samples for SMOTE, using basic balancing")
                result = balance_dataset(X_train, y_train)
                log_timing(f"SMOTE fallback to basic", t0)
                return result
            
            smote = SMOTE(random_state=DEFAULT_RANDOM_STATE, k_neighbors=k_neighbors)
            X_balanced, y_balanced = smote.fit_resample(X_train, y_train)
            
            # Convert back to pandas
            X_balanced = pd.DataFrame(X_balanced, columns=X_train.columns)
            y_balanced = pd.Series(y_balanced, name=y_train.name or 'target')
            
        elif method == 'smoteenn':
            # SMOTE + Edited Nearest Neighbours for combined over/under sampling
            k_neighbors = min(DEFAULT_SMOTE_K_NEIGHBORS, min_class_count - 1) if min_class_count > 1 else 1
            
            if min_class_count < 2:
                logger.warning("Not enough minority samples for SMOTEENN, using basic balancing")
                result = balance_dataset(X_train, y_train)
                log_timing(f"SMOTEENN fallback to basic", t0)
                return result
            
            smoteenn = SMOTEENN(random_state=DEFAULT_RANDOM_STATE, smote=SMOTE(k_neighbors=k_neighbors))
            X_balanced, y_balanced = smoteenn.fit_resample(X_train, y_train)
            
            # Convert back to pandas
            X_balanced = pd.DataFrame(X_balanced, columns=X_train.columns)
            y_balanced = pd.Series(y_balanced, name=y_train.name or 'target')
            
        elif method == 'combined':
            # Custom combined approach: Under-sample majority + SMOTE minority
            majority_class = original_counts.idxmax()
            minority_class = original_counts.idxmin()
            majority_count = original_counts[majority_class]
            minority_count = original_counts[minority_class]
            
            if minority_count < 2:
                logger.warning("Not enough minority samples for combined approach, using basic")
                result = balance_dataset(X_train, y_train)
                log_timing(f"Combined fallback to basic", t0)
                return result
            
            # Step 1: Under-sample majority to 3:1 ratio
            target_majority = min(majority_count, minority_count * 3)
            
            majority_indices = y_train[y_train == majority_class].index
            minority_indices = y_train[y_train == minority_class].index
            
            # Random under-sampling of majority
            sampled_majority_indices = np.random.choice(
                majority_indices, size=target_majority, replace=False
            )
            
            # Combine with all minority samples
            combined_indices = np.concatenate([sampled_majority_indices, minority_indices])
            X_intermediate = X_train.loc[combined_indices]
            y_intermediate = y_train.loc[combined_indices]
            
            # Step 2: Apply SMOTE to balance
            k_neighbors = min(DEFAULT_SMOTE_K_NEIGHBORS, minority_count - 1)
            smote = SMOTE(random_state=DEFAULT_RANDOM_STATE, k_neighbors=k_neighbors)
            X_balanced, y_balanced = smote.fit_resample(X_intermediate, y_intermediate)
            
            # Convert back to pandas
            X_balanced = pd.DataFrame(X_balanced, columns=X_train.columns)
            y_balanced = pd.Series(y_balanced, name=y_train.name or 'target')
            
        else:
            logger.warning(f"Unknown method '{method}', falling back to basic balancing")
            result = balance_dataset(X_train, y_train)
            log_timing(f"Unknown method fallback to basic", t0)
            return result
        
        # Log new distribution
        new_counts = y_balanced.value_counts().sort_index()
        logger.info(f"Balanced class distribution: {new_counts.to_dict()}")
        logger.info(f"Dataset size: {len(X_train)} â†’ {len(X_balanced)} samples")
        
        log_timing(f"{method.upper()} balancing", t0)
        return X_balanced, y_balanced
        
    except Exception as e:
        logger.error(f"Error in {method} balancing: {e}")
        logger.info("Falling back to basic balancing")
        result = balance_dataset(X_train, y_train)
        log_timing(f"{method} error fallback to basic", t0)
        return result


def create_calibrated_model(base_estimator, X_train: pd.DataFrame, y_train: pd.Series, cv_folds: int = 3):
    """
    Create and train a calibrated version of a model using CalibratedClassifierCV.
    
    Args:
        base_estimator: The base estimator to calibrate (sklearn-compatible)
        X_train: Training features
        y_train: Training labels
        cv_folds: Number of CV folds (will be adapted for small datasets)
        
    Returns:
        Trained calibrated model or trained base model if calibration fails
    """
    if not CALIBRATION_AVAILABLE:
        logger.warning(f"Calibration not available, using base model")
        base_estimator.fit(X_train, y_train)
        return base_estimator
    
    try:
        # Adaptive CV folds for small datasets
        adapted_cv = min(cv_folds, 3)  # Use max 3 folds for small datasets
        
        # Use isotonic regression for better performance with limited data
        calibrated_model = CalibratedClassifierCV(
            base_estimator, 
            method='isotonic',  # Better for small datasets than 'sigmoid'
            cv=adapted_cv
        )
        
        # Fit the calibrated model
        calibrated_model.fit(X_train, y_train)
        
        logger.info(f"Created and trained calibrated model with {adapted_cv} CV folds")
        return calibrated_model
        
    except Exception as e:
        logger.warning(f"Failed to calibrate model: {e}")
        # Fall back to base estimator
        base_estimator.fit(X_train, y_train)
        return base_estimator


def balance_dataset(X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Balance the dataset with a controlled approach to handle class imbalance.

    The function first caps extreme class ratios, then either applies
    modest oversampling to achieve roughly a 60:40 majority/minority
    split or falls back to a simple ``RandomOverSampler`` when the
    minority class is extremely small.

    Args:
        X_train: Training features
        y_train: Training labels

    Returns:
        ``(X_train_balanced, y_train_balanced)`` â€“ the balanced dataset
    """
    # Check if we have multiple classes
    unique_classes = y_train.unique()
    if len(unique_classes) < 2:
        logger.warning(f"Only one class found in balance_dataset: {unique_classes}. Returning original data.")
        return X_train.copy(), y_train.copy()
    
    # Log the original distribution
    try:
        counts = y_train.value_counts().to_dict()
        logger.info(f"Original class distribution: {counts}")
    except Exception:
        logger.info("Could not compute original class distribution")
    
    X_capped, y_capped = cap_majority_ratio(X_train, y_train, max_ratio=DEFAULT_MAX_RATIO)
    
    # Check again after capping
    unique_classes_capped = np.unique(y_capped)
    if len(unique_classes_capped) < 2:
        logger.warning(f"Only one class after capping: {unique_classes_capped}. Returning original data.")
        return X_train.copy(), y_train.copy()
    
    # Determine class counts
    class_0 = X_capped[y_capped == 0] if hasattr(X_capped, '__getitem__') else X_train[y_train == 0]
    class_1 = X_capped[y_capped == 1] if hasattr(X_capped, '__getitem__') else X_train[y_train == 1]
    num_class_0 = len(class_0)
    num_class_1 = len(class_1)
    
    # If any class is empty, return original data
    if num_class_0 == 0 or num_class_1 == 0:
        logger.warning(f"Empty class detected (class_0: {num_class_0}, class_1: {num_class_1}). Returning original data.")
        return X_train.copy(), y_train.copy()
    
    # If the minority class is tiny, oversample using RandomOverSampler
    if min(num_class_0, num_class_1) < DEFAULT_MINORITY_THRESHOLD:
        logger.info("Small minority class detected, using oversampling")
        if IMBLEARN_AVAILABLE:
            try:
                ros = RandomOverSampler(random_state=DEFAULT_RANDOM_STATE)
                X_bal, y_bal = ros.fit_resample(X_train, y_train)
                return pd.DataFrame(X_bal, columns=X_train.columns), pd.Series(y_bal, name=y_train.name)
            except Exception as e:
                logger.warning(f"RandomOverSampler failed: {e}. Using manual oversampling.")
                # Fall through to manual oversampling below
        
        # Fallback oversampling: duplicate minority class samples to approximate balance
        minority_class = 0 if num_class_0 < num_class_1 else 1
        X_min = X_train[y_train == minority_class]
        y_min = y_train[y_train == minority_class]
        
        if len(X_min) == 0:
            logger.warning("Minority class is empty. Returning original data.")
            return X_train.copy(), y_train.copy()
        
        n_samples = abs(num_class_0 - num_class_1)
        if n_samples > 0:
            idx = np.random.choice(len(X_min), size=n_samples, replace=True)
            X_oversampled = pd.concat([X_train, X_min.iloc[idx]]).reset_index(drop=True)
            y_oversampled = pd.concat([y_train, y_min.iloc[idx]]).reset_index(drop=True)
            return X_oversampled, y_oversampled
        else:
            return X_train.copy(), y_train.copy()
    
    # Controlled balancing
    if num_class_0 > num_class_1:
        # Class 0 is majority
        target_class_0 = min(num_class_0, int(num_class_1 / (1 - DEFAULT_DESIRED_RATIO) * DEFAULT_DESIRED_RATIO))
        class_0_down = class_0.sample(target_class_0, random_state=DEFAULT_RANDOM_STATE) if hasattr(class_0, 'sample') else class_0[:target_class_0]
        X_bal = pd.concat([class_1, class_0_down])
        y_bal = [1] * len(class_1) + [0] * len(class_0_down)
    else:
        # Class 1 is majority
        target_class_1 = min(num_class_1, int(num_class_0 / (1 - DEFAULT_DESIRED_RATIO) * DEFAULT_DESIRED_RATIO))
        class_1_down = class_1.sample(target_class_1, random_state=DEFAULT_RANDOM_STATE) if hasattr(class_1, 'sample') else class_1[:target_class_1]
        X_bal = pd.concat([class_0, class_1_down])
        y_bal = [0] * len(class_0) + [1] * len(class_1_down)
    
    y_balanced = pd.Series(y_bal, name=y_train.name if hasattr(y_train, 'name') else 'target')
    # Shuffle the balanced dataset
    X_balanced = X_bal.copy()
    X_balanced['y'] = y_balanced.values
    X_balanced = X_balanced.sample(frac=1, random_state=DEFAULT_RANDOM_STATE).reset_index(drop=True)
    y_balanced = X_balanced.pop('y')
    return X_balanced, y_balanced


def out_of_fold_stacking(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, 
                        models_config: Dict[str, Any], n_folds: int = 3) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
    """
    Perform out-of-fold stacking to prevent overfitting in meta-model training.
    
    Args:
        X_train: Training features
        y_train: Training labels
        X_test: Test features
        models_config: Dictionary with model configurations
        n_folds: Number of cross-validation folds
        
    Returns:
        Tuple of (train_meta_features, test_meta_features, trained_models)
    """
    logger.info(f"Starting out-of-fold stacking with {n_folds} folds")
    
    # Adaptive folds for small datasets
    actual_folds = min(n_folds, len(y_train) // 10, 5)  # At least 10 samples per fold
    if len(np.unique(y_train)) > 1:
        # Ensure each fold has both classes
        min_class_count = min(y_train.value_counts())
        actual_folds = min(actual_folds, min_class_count)
    
    actual_folds = max(2, actual_folds)  # At least 2 folds
    logger.info(f"Using {actual_folds} folds for stacking")
    
    # Initialize meta-feature arrays
    n_models = len(models_config)
    train_meta_features = np.zeros((len(X_train), n_models))
    test_meta_features = np.zeros((len(X_test), n_models))
    
    # Cross-validation setup
    skf = StratifiedKFold(n_splits=actual_folds, shuffle=True, random_state=42)
    trained_models = {}
    
    for model_idx, (model_name, model_info) in enumerate(models_config.items()):
        logger.info(f"Training {model_name} with out-of-fold stacking...")
        
        fold_predictions = []
        fold_models = []
        
        # Cross-validation loop
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
            logger.info(f"  Fold {fold + 1}/{actual_folds}")
            
            # Split data
            X_fold_train = X_train.iloc[train_idx]
            y_fold_train = y_train.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx]
            
            # Balance training data for this fold
            balance_method = model_info.get('balance_method', 'smote')
            X_fold_balanced, y_fold_balanced = prepare_balanced_data(X_fold_train, y_fold_train, balance_method)
            
            # Train model
            model_class = model_info['class']
            fold_model = model_class()
            fold_model.train(X_fold_balanced, y_fold_balanced)
            
            # Apply calibration if specified
            if model_info.get('calibrate', False):
                fold_model = create_calibrated_model(fold_model, f"{model_name}_fold_{fold}")
                # Re-fit calibrated model
                if hasattr(fold_model, 'fit'):  # CalibratedClassifierCV
                    fold_model.fit(X_fold_balanced, y_fold_balanced)
            
            # Predict on validation set
            val_proba = fold_model.predict_proba(X_fold_val)
            val_proba_1 = val_proba[:, -1] if val_proba.ndim > 1 else val_proba
            
            # Store predictions for this fold
            train_meta_features[val_idx, model_idx] = val_proba_1
            
            # Predict on test set
            test_proba = fold_model.predict_proba(X_test)
            test_proba_1 = test_proba[:, -1] if test_proba.ndim > 1 else test_proba
            fold_predictions.append(test_proba_1)
            
            fold_models.append(fold_model)
        
        # Average test predictions across folds
        test_meta_features[:, model_idx] = np.mean(fold_predictions, axis=0)
        
        # Store fold models
        trained_models[model_name] = {
            'models': fold_models,
            'config': model_info
        }
        
        logger.info(f"  Completed {model_name} stacking")
    
    logger.info("Out-of-fold stacking completed")
    return train_meta_features, test_meta_features, trained_models


def clean_data_for_ml(X: pd.DataFrame) -> pd.DataFrame:
    """
    Clean the feature matrix by handling infinite and extreme values.

    Infinite values are replaced with NaN.  Numeric columns are clipped
    to the 1st and 99th percentile to mitigate extreme outliers.  The
    function returns a copy of the cleaned feature matrix.
    """
    X_clean = X.copy()
    # Replace infinite values with NaN
    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
    # Clip extreme values
    for col in X_clean.columns:
        if X_clean[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            q1 = X_clean[col].quantile(0.01)
            q99 = X_clean[col].quantile(0.99)
            X_clean[col] = X_clean[col].clip(lower=q1, upper=q99)
    return X_clean


def prepare_training_data(df: pd.DataFrame, feature_columns: List[str], target_column: str = 'target') -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Prepare training and test splits with guaranteed class representation in both sets.
    """
    logger.info("Preparing training data with improved class balance handling...")
    X = df[feature_columns].copy()
    y = df[target_column].copy()
    X_clean = clean_data_for_ml(X)
    mask = ~(X_clean.isnull().any(axis=1) | y.isnull())
    X_final = X_clean[mask]
    y_final = y[mask]
    df_final = df[mask]
    
    # Check class distribution
    class_counts = y_final.value_counts()
    logger.info(f"Total class distribution: {class_counts.to_dict()}")
    
    # If we have very few total samples of minority class, use a smaller test size
    minority_count = class_counts.min()
    if minority_count < 4:
        logger.warning(f"Very few minority samples ({minority_count}). Using minimal test set.")
        test_size = max(0.1, 1/len(y_final))  # At least 1 sample in test, max 10%
    elif minority_count < 10:
        test_size = 0.15  # Smaller test set to preserve minority samples
    else:
        test_size = 0.2   # Standard test size
    
    # Force stratified split to ensure both classes in both sets
    max_attempts = 5
    for attempt in range(max_attempts):
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X_final, y_final, 
                test_size=test_size, 
                random_state=42 + attempt,  # Different seed for each attempt
                stratify=y_final
            )
            
            # Verify both sets have both classes
            train_classes = set(y_train.unique())
            test_classes = set(y_test.unique())
            
            if len(train_classes) >= 2 and len(test_classes) >= 2:
                logger.info(f"âœ… Successful split on attempt {attempt + 1}")
                logger.info(f"   Train classes: {sorted(train_classes)}")
                logger.info(f"   Test classes: {sorted(test_classes)}")
                logger.info(f"   Train distribution: {y_train.value_counts().to_dict()}")
                logger.info(f"   Test distribution: {y_test.value_counts().to_dict()}")
                return X_train, X_test, y_train, y_test
            else:
                logger.warning(f"Attempt {attempt + 1}: Split resulted in missing classes")
                
        except ValueError as e:
            logger.warning(f"Attempt {attempt + 1}: Stratified split failed: {e}")
            # If stratify fails, reduce test size further
            test_size = max(0.05, test_size * 0.8)
    
    # If all stratified attempts fail, use manual splitting to guarantee class representation
    logger.warning("Stratified splitting failed. Using manual class-preserving split.")
    
    # Separate by class
    class_0_indices = y_final[y_final == 0].index
    class_1_indices = y_final[y_final == 1].index
    
    # Ensure at least 1 sample of each class in test set
    n_test_class_0 = max(1, int(len(class_0_indices) * test_size))
    n_test_class_1 = max(1, int(len(class_1_indices) * test_size))
    
    # Random split for each class
    np.random.seed(42)
    test_indices_0 = np.random.choice(class_0_indices, size=n_test_class_0, replace=False)
    test_indices_1 = np.random.choice(class_1_indices, size=n_test_class_1, replace=False)
    
    test_indices = np.concatenate([test_indices_0, test_indices_1])
    train_indices = np.setdiff1d(y_final.index, test_indices)
    
    X_train = X_final.loc[train_indices]
    X_test = X_final.loc[test_indices]
    y_train = y_final.loc[train_indices]
    y_test = y_final.loc[test_indices]
    
    logger.info(f"âœ… Manual split completed:")
    logger.info(f"   Train distribution: {y_train.value_counts().to_dict()}")
    logger.info(f"   Test distribution: {y_test.value_counts().to_dict()}")
    
    return X_train, X_test, y_train, y_test

def train_committee_models_advanced(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> Tuple[Dict[str, Any], Dict[str, Dict[str, float]]]:
    """
    Train Committee of Five with out-of-fold stacking procedure.
    
    Uses StratifiedKFold(n_splits=5) with SMOTEENN oversampling, CalibratedClassifierCV, 
    and optimal threshold tuning for each base model. Implements true out-of-fold stacking
    to prevent overfitting in meta-model training.
    
    Args:
        X_train: Training features
        y_train: Training labels
        X_test: Test features
        y_test: Test labels
        
    Returns:
        Tuple of (models dict, metrics dict) where models contains trained model info
        and metrics contains performance metrics for each model
    """
    logger.info("ðŸš€ Starting Out-of-Fold Committee of Five training...")
    logger.info("ðŸ“Š Implementing: SMOTEENN, Calibration, Threshold Optimization, True OOF Stacking")
    
    # Check if we have both classes in training data
    unique_classes = y_train.unique()
    if len(unique_classes) < 2:
        logger.error(f"Cannot perform training with only one class: {unique_classes}")
        raise ValueError(f"Training data contains only one class: {unique_classes}. Need at least 2 classes for binary classification.")
    
    # Check if SMOTEENN is available
    if not ADVANCED_SAMPLING_AVAILABLE:
        logger.error("SMOTEENN not available. Install with: pip install imbalanced-learn")
        raise ImportError("SMOTEENN required for this training procedure")
    
    if not CALIBRATION_AVAILABLE:
        logger.error("CalibratedClassifierCV not available. Update sklearn.")
        raise ImportError("CalibratedClassifierCV required for this training procedure")
    
    # 1. OOF stacking - Initialize StratifiedKFold with configurable splits
    logger.info(f"ðŸ“‚ Setting up {DEFAULT_N_FOLDS}-fold stratified cross-validation...")
    log_timing("Starting out-of-fold stacking setup", enable=ENABLE_TIMING)
    skf = StratifiedKFold(n_splits=DEFAULT_N_FOLDS, shuffle=DEFAULT_SHUFFLE, random_state=DEFAULT_RANDOM_STATE)
    
    # Define base model classes with optimized eval metrics
    base_model_classes = {
        'xgboost': XGBoostModel,
        'lightgbm': LightGBMModel, 
        'catboost': CatBoostModel,
        'random_forest': RandomForestModel,
        'svm': SVMClassifier
    }
    
    # Initialize out-of-fold predictions storage
    n_samples = len(X_train)
    oof_preds = {model_name: np.zeros(n_samples) for model_name in base_model_classes.keys()}
    test_preds = {model_name: [] for model_name in base_model_classes.keys()}
    model_thresholds = {}
    trained_models = {model_name: [] for model_name in base_model_classes.keys()}
    
    logger.info(f"ðŸ”„ Starting out-of-fold training on {n_samples} samples...")
    log_timing(f"Beginning {DEFAULT_N_FOLDS}-fold training", enable=ENABLE_TIMING)
    
    # Iterate through each fold
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
        logger.info(f"ðŸ“‹ Processing Fold {fold_idx + 1}/{DEFAULT_N_FOLDS}...")
        log_timing(f"Starting fold {fold_idx + 1}", enable=ENABLE_TIMING)
        
        # Split data for this fold
        X_fold_train = X_train.iloc[train_idx]
        y_fold_train = y_train.iloc[train_idx]
        X_fold_val = X_train.iloc[val_idx]
        y_fold_val = y_train.iloc[val_idx]
        
        # Check if fold has both classes
        fold_classes = y_fold_train.unique()
        if len(fold_classes) < 2:
            logger.warning(f"  âš ï¸  Fold {fold_idx + 1} has only one class: {fold_classes}. Using original data.")
            X_res, y_res = X_fold_train, y_fold_train
        else:
            # 2. Oversample with SMOTEENN
            logger.info(f"  ðŸ”„ Oversampling fold {fold_idx + 1} with SMOTEENN...")
            try:
                # Use adaptive k_neighbors for small datasets
                minority_count = min(y_fold_train.value_counts())
                k_neighbors = min(DEFAULT_SMOTE_K_NEIGHBORS, minority_count - 1) if minority_count > 1 else 1
                
                smoteenn = SMOTEENN(
                    random_state=DEFAULT_RANDOM_STATE,
                    smote=SMOTE(k_neighbors=k_neighbors, random_state=DEFAULT_RANDOM_STATE)
                )
                X_res, y_res = smoteenn.fit_resample(X_fold_train, y_fold_train)
                
                # Convert back to pandas
                X_res = pd.DataFrame(X_res, columns=X_fold_train.columns)
                y_res = pd.Series(y_res, name=y_fold_train.name or 'target')
                
                logger.info(f"    ðŸ“Š Fold {fold_idx + 1}: {len(X_fold_train)} â†’ {len(X_res)} samples")
                
            except Exception as e:
                logger.warning(f"    âš ï¸  SMOTEENN failed for fold {fold_idx + 1}: {e}. Using original data.")
                X_res, y_res = X_fold_train, y_fold_train
        
        # Train each base model on this fold
        for model_name, model_class in base_model_classes.items():
            logger.info(f"    ðŸ¤– Training {model_name} on fold {fold_idx + 1}...")
            
            # 3. Calibrate each model - instantiate base model and get sklearn estimator
            base_model = model_class()
            
            # Extract the underlying sklearn estimator from our custom wrapper
            sklearn_estimator = None
            if hasattr(base_model, 'model') and base_model.model is not None:
                # XGBoost, LightGBM, CatBoost, Random Forest use .model attribute
                sklearn_estimator = base_model.model
            elif hasattr(base_model, 'pipeline') and base_model.pipeline is not None:
                # SVM uses .pipeline attribute
                sklearn_estimator = base_model.pipeline
            else:
                logger.error(f"      âŒ Cannot extract sklearn estimator from {model_name}")
                continue
            
            # Wrap sklearn estimator in CalibratedClassifierCV with adaptive calibration
            # Adaptive CV folds based on available data after SMOTEENN
            min_class_samples = min(y_res.value_counts()) if len(y_res.value_counts()) > 1 else len(y_res)
            adaptive_cv = min(DEFAULT_CALIBRATION_CV, max(2, min_class_samples // 2))
            
            cal_model = CalibratedClassifierCV(
                estimator=sklearn_estimator,  # Use underlying sklearn estimator
                method=DEFAULT_CALIBRATION_METHOD,
                cv=adaptive_cv
            )
            
            logger.info(f"      ðŸ“Š Using {adaptive_cv}-fold calibration CV for {model_name}")
            
            # Train calibrated model on oversampled data
            try:
                # Check if we have enough samples for calibration
                if len(y_res) < 4:  # Need at least 4 samples for 2-fold CV
                    logger.warning(f"      âš ï¸  Too few samples ({len(y_res)}) for calibration. Using base model.")
                    # Train base model directly without calibration
                    sklearn_estimator.fit(X_res, y_res)
                    cal_model = sklearn_estimator
                else:
                    cal_model.fit(X_res, y_res)
                
                logger.info(f"      âœ… {model_name} calibrated model trained")
            except Exception as e:
                logger.error(f"      âŒ Failed to train {model_name}: {e}")
                # Try with base model as fallback
                try:
                    logger.warning(f"      ðŸ”„ Falling back to base model for {model_name}")
                    sklearn_estimator.fit(X_res, y_res)
                    cal_model = sklearn_estimator
                    logger.info(f"      âœ… {model_name} base model trained (no calibration)")
                except Exception as e2:
                    logger.error(f"      âŒ Failed to train base model for {model_name}: {e2}")
                    continue
            
            # Predict probabilities on validation set
            try:
                val_proba = cal_model.predict_proba(X_fold_val)[:, 1]
            except Exception as e:
                logger.error(f"      âŒ Failed to predict with {model_name}: {e}")
                continue
            
            # 4. Threshold tuning - find optimal threshold for this fold
            try:
                if len(y_fold_val.unique()) < 2:
                    logger.warning(f"      âš ï¸  Validation fold has only one class. Using default threshold.")
                    optimal_threshold = 0.5
                else:
                    optimal_threshold, _ = find_optimal_threshold(y_fold_val, val_proba, metric='f1')
                    logger.info(f"      ðŸŽ¯ {model_name} optimal threshold: {optimal_threshold:.3f}")
            except Exception as e:
                logger.warning(f"      âš ï¸  Threshold optimization failed for {model_name}: {e}")
                optimal_threshold = 0.5
            
            # Store out-of-fold predictions
            oof_preds[model_name][val_idx] = val_proba
            
            # Predict on test set and store
            try:
                test_proba = cal_model.predict_proba(X_test)[:, 1]
                test_preds[model_name].append(test_proba)
            except Exception as e:
                logger.error(f"      âŒ Failed to predict test set with {model_name}: {e}")
                test_preds[model_name].append(np.zeros(len(X_test)))
            
            # Store trained model and threshold
            trained_models[model_name].append(cal_model)
            if model_name not in model_thresholds:
                model_thresholds[model_name] = []
            model_thresholds[model_name].append(optimal_threshold)
        
        # Log fold completion timing
        log_timing(f"Completed fold {fold_idx + 1}", enable=ENABLE_TIMING)
    
    # Calculate average thresholds across folds
    log_timing("Starting threshold averaging and test prediction aggregation", enable=ENABLE_TIMING)
    avg_thresholds = {}
    for model_name in base_model_classes.keys():
        if model_name in model_thresholds and len(model_thresholds[model_name]) > 0:
            avg_thresholds[model_name] = np.mean(model_thresholds[model_name])
            logger.info(f"ðŸŽ¯ {model_name} average threshold: {avg_thresholds[model_name]:.3f}")
        else:
            avg_thresholds[model_name] = 0.5
    
    # Average test predictions across folds
    avg_test_preds = {}
    for model_name in base_model_classes.keys():
        if test_preds[model_name]:
            avg_test_preds[model_name] = np.mean(test_preds[model_name], axis=0)
        else:
            avg_test_preds[model_name] = np.zeros(len(X_test))
    
    # 5. Train meta-model - assemble meta-features from OOF predictions
    logger.info("ðŸ”— Training meta-model on out-of-fold predictions...")
    log_timing("Starting meta-model training", enable=ENABLE_TIMING)
    
    meta_X_train = pd.DataFrame(oof_preds)
    logger.info(f"ðŸ“Š Meta-features shape: {meta_X_train.shape}")
    
    # Train logistic regression meta-model with configurable parameters
    meta_model = LogisticRegression(
        random_state=DEFAULT_RANDOM_STATE, 
        max_iter=DEFAULT_META_MAX_ITER
    )
    meta_model.fit(meta_X_train, y_train)
    logger.info("âœ… Meta-model trained successfully")
    log_timing("Meta-model training completed", enable=ENABLE_TIMING)
    
    # 6. Final evaluation - prepare test predictions
    logger.info("ðŸ“Š Evaluating models on test set...")
    log_timing("Starting final evaluation", enable=ENABLE_TIMING)
    
    # For base models: binarize test predictions with optimal thresholds
    base_test_binary = {}
    for model_name in base_model_classes.keys():
        threshold = avg_thresholds[model_name]
        test_proba = avg_test_preds[model_name]
        base_test_binary[model_name] = (test_proba >= threshold).astype(int)
    
    # Create meta test features from binary predictions
    meta_X_test = pd.DataFrame(base_test_binary)
    
    # Meta-model final prediction
    y_pred_meta = meta_model.predict(meta_X_test)
    
    # Calculate metrics for all models
    metrics = {}
    
    # Base model metrics
    for model_name in base_model_classes.keys():
        y_pred_base = base_test_binary[model_name]
        test_proba = avg_test_preds[model_name]
        
        try:
            metrics[model_name] = {
                'accuracy': accuracy_score(y_test, y_pred_base),
                'precision': precision_score(y_test, y_pred_base, zero_division=0),
                'recall': recall_score(y_test, y_pred_base, zero_division=0),
                'f1': f1_score(y_test, y_pred_base, zero_division=0),
                'roc_auc': roc_auc_score(y_test, test_proba) if len(np.unique(y_test)) > 1 else 0.0,
                'pr_auc': average_precision_score(y_test, test_proba) if len(np.unique(y_test)) > 1 else 0.0
            }
            
            logger.info(f"âœ… {model_name} - F1: {metrics[model_name]['f1']:.3f}, "
                       f"ROC-AUC: {metrics[model_name]['roc_auc']:.3f}, "
                       f"PR-AUC: {metrics[model_name]['pr_auc']:.3f}")
                       
        except Exception as e:
            logger.error(f"âŒ Error calculating metrics for {model_name}: {e}")
            metrics[model_name] = {
                'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0,
                'f1': 0.0, 'roc_auc': 0.0, 'pr_auc': 0.0
            }
    
    # Meta-model metrics
    try:
        meta_proba = meta_model.predict_proba(meta_X_test)[:, 1]
        
        metrics['meta_model'] = {
            'accuracy': accuracy_score(y_test, y_pred_meta),
            'precision': precision_score(y_test, y_pred_meta, zero_division=0),
            'recall': recall_score(y_test, y_pred_meta, zero_division=0),
            'f1': f1_score(y_test, y_pred_meta, zero_division=0),
            'roc_auc': roc_auc_score(y_test, meta_proba) if len(np.unique(y_test)) > 1 else 0.0,
            'pr_auc': average_precision_score(y_test, meta_proba) if len(np.unique(y_test)) > 1 else 0.0
        }
        
        logger.info(f"ðŸ† Meta-model - F1: {metrics['meta_model']['f1']:.3f}, "
                   f"ROC-AUC: {metrics['meta_model']['roc_auc']:.3f}, "
                   f"PR-AUC: {metrics['meta_model']['pr_auc']:.3f}")
                   
    except Exception as e:
        logger.error(f"âŒ Error calculating meta-model metrics: {e}")
        metrics['meta_model'] = {
            'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0,
            'f1': 0.0, 'roc_auc': 0.0, 'pr_auc': 0.0
        }
    
    # Store model information
    models = {
        'base_models': trained_models,
        'meta_model': meta_model,
        'thresholds': avg_thresholds,
        'oof_predictions': oof_preds,
        'test_predictions': avg_test_preds,
        'meta_test_features': meta_X_test,
        'meta_predictions': y_pred_meta
    }
    
    # Log final summary
    f1_scores = [metrics[m]['f1'] for m in metrics.keys()]
    avg_f1 = np.mean(f1_scores)
    best_f1 = max(f1_scores)
    best_model = max(metrics.keys(), key=lambda k: metrics[k]['f1'])
    
    logger.info(f"ðŸŽ¯ Out-of-Fold Training Summary:")
    logger.info(f"   Average F1 Score: {avg_f1:.3f}")
    logger.info(f"   Best F1 Score: {best_f1:.3f} ({best_model})")
    logger.info(f"   Meta-model F1 Score: {metrics['meta_model']['f1']:.3f}")
    
    log_timing("Out-of-fold training completed", enable=ENABLE_TIMING)
    
    return models, metrics


def prepare_sampler(method: str, random_state: int = DEFAULT_RANDOM_STATE):
    """
    Prepare sampling method for class imbalance handling.
    
    Args:
        method: Sampling method ('smoteenn' or 'none')
        random_state: Random state for reproducibility
        
    Returns:
        Configured sampler or None
    """
    if method == 'smoteenn':
        try:
            from imblearn.combine import SMOTEENN
            from imblearn.over_sampling import SMOTE
            return SMOTEENN(random_state=random_state)
        except ImportError:
            logger.warning("SMOTEENN not available. Using basic balancing instead.")
            return None
    return None


def evaluate_models_comprehensive(fitted_models: Dict[str, Any], thresholds: Dict[str, float], 
                                X_test: pd.DataFrame, y_test: pd.Series, meta_model: Any = None) -> Dict[str, Dict[str, float]]:
    """
    Comprehensive evaluation of base models and meta-model with all metrics.
    
    Args:
        fitted_models: Dictionary of trained models
        thresholds: Dictionary of optimal thresholds per model
        X_test: Test features
        y_test: Test labels
        meta_model: Trained meta-model (optional)
        
    Returns:
        Dictionary of metrics for each model
    """
    from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                                f1_score, roc_auc_score, average_precision_score)
    
    results = {}
    
    # Evaluate base models
    for name, model in fitted_models.items():
        if name == 'meta_model':
            continue
            
        try:
            # Get predictions
            if hasattr(model, 'predict_proba'):
                proba = model.predict_proba(X_test)[:, 1]
            else:
                proba = model.predict(X_test)
                
            thresh = thresholds.get(name, 0.5)
            y_pred = (proba >= thresh).astype(int)

            results[name] = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, zero_division=0),
                'recall': recall_score(y_test, y_pred, zero_division=0),
                'f1': f1_score(y_test, y_pred, zero_division=0),
                'roc_auc': roc_auc_score(y_test, proba) if len(np.unique(y_test)) > 1 else 0.0,
                'pr_auc': average_precision_score(y_test, proba) if len(np.unique(y_test)) > 1 else 0.0
            }
            
        except Exception as e:
            logger.error(f"Error evaluating model {name}: {e}")
            results[name] = {
                'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0,
                'f1': 0.0, 'roc_auc': 0.0, 'pr_auc': 0.0
            }

    # Evaluate meta-model if provided
    if meta_model is not None:
        try:
            # Create meta-features from base model predictions
            meta_X_test = pd.DataFrame({
                name: fitted_models[name].predict_proba(X_test)[:, 1] 
                for name in fitted_models.keys() if name != 'meta_model'
            })
            
            meta_proba = meta_model.predict_proba(meta_X_test)[:, 1]
            y_meta = (meta_proba >= 0.5).astype(int)  # Default threshold for meta-model
            
            results['meta_model'] = {
                'accuracy': accuracy_score(y_test, y_meta),
                'precision': precision_score(y_test, y_meta, zero_division=0),
                'recall': recall_score(y_test, y_meta, zero_division=0),
                'f1': f1_score(y_test, y_meta, zero_division=0),
                'roc_auc': roc_auc_score(y_test, meta_proba) if len(np.unique(y_test)) > 1 else 0.0,
                'pr_auc': average_precision_score(y_test, meta_proba) if len(np.unique(y_test)) > 1 else 0.0
            }
            
        except Exception as e:
            logger.error(f"Error evaluating meta-model: {e}")
            results['meta_model'] = {
                'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0,
                'f1': 0.0, 'roc_auc': 0.0, 'pr_auc': 0.0
            }

    return results


def prepare_calibration_method(method: str):
    """
    Prepare calibration method for probability calibration.
    
    Args:
        method: Calibration method ('isotonic', 'sigmoid', or 'none')
        
    Returns:
        Calibration method string or None
    """
    if method in ('isotonic', 'sigmoid'):
        return method
    return None


def train_and_calibrate_model(model, X_train, y_train, X_val, y_val, calibration_method, use_threshold=True):
    """
    Train and calibrate a single model with optimal threshold tuning.
    
    Args:
        model: Base model instance
        X_train: Training features
        y_train: Training labels
        X_val: Validation features
        y_val: Validation labels
        calibration_method: Calibration method ('isotonic', 'sigmoid', or None)
        use_threshold: Whether to tune threshold
        
    Returns:
        Tuple of (fitted_model, best_threshold)
    """
    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.metrics import f1_score
    
    # Extract the underlying sklearn estimator from our custom wrapper
    sklearn_estimator = None
    if hasattr(model, 'model') and model.model is not None:
        sklearn_estimator = model.model
    elif hasattr(model, 'pipeline') and model.pipeline is not None:
        sklearn_estimator = model.pipeline
    else:
        # For custom models, try to get the sklearn-compatible estimator
        sklearn_estimator = getattr(model, 'estimator', model)
    
    # Apply calibration if specified
    if calibration_method and CALIBRATION_AVAILABLE:
        # Adaptive CV folds based on available validation data
        min_class_samples = min(y_val.value_counts()) if len(y_val.value_counts()) > 1 else len(y_val)
        adaptive_cv = min(DEFAULT_CALIBRATION_CV, max(2, min_class_samples // 2))
        
        cal_model = CalibratedClassifierCV(
            estimator=sklearn_estimator,
            method=calibration_method,
            cv=adaptive_cv
        )
        fitted_model = cal_model
    else:
        fitted_model = sklearn_estimator
    
    # Train the model
    fitted_model.fit(X_train, y_train)
    
    # Tune threshold if requested
    best_threshold = 0.5  # Default
    if use_threshold:
        val_proba = fitted_model.predict_proba(X_val)[:, 1]
        
        # Grid search for optimal threshold
        thresholds = np.arange(0.1, 0.9, 0.05)
        best_f1 = 0
        
        for threshold in thresholds:
            y_pred_thresh = (val_proba >= threshold).astype(int)
            f1 = f1_score(y_val, y_pred_thresh, zero_division=0)
            
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
    
    return fitted_model, best_threshold


def export_metrics_to_csv(metrics: Dict[str, Dict[str, float]], thresholds: Dict[str, float], output_dir: str) -> None:
    """
    Export model metrics to CSV file.
    
    Args:
        metrics: Dictionary of metrics for each model
        thresholds: Dictionary of optimal thresholds for each model
        output_dir: Output directory for the CSV file
    """
    try:
        # Prepare data for CSV
        csv_data = []
        for model_name, model_metrics in metrics.items():
            row = {
                'model_name': model_name,
                'f1_score': model_metrics.get('f1', 0.0),
                'roc_auc': model_metrics.get('roc_auc', 0.0),
                'pr_auc': model_metrics.get('pr_auc', 0.0),
                'optimal_threshold': thresholds.get(model_name, 0.5),
                'accuracy': model_metrics.get('accuracy', 0.0),
                'precision': model_metrics.get('precision', 0.0),
                'recall': model_metrics.get('recall', 0.0)
            }
            csv_data.append(row)
        
        # Create DataFrame and save to CSV
        import pandas as pd
        df_metrics = pd.DataFrame(csv_data)
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        csv_path = os.path.join(output_dir, 'model_summary.csv')
        df_metrics.to_csv(csv_path, index=False)
        logger.info(f"ðŸ“Š Model metrics exported to: {csv_path}")
        
    except Exception as e:
        logger.error(f"âŒ Error exporting metrics to CSV: {e}")


def save_meta_model_coefficients(meta_model, base_model_names: List[str], output_dir: str) -> None:
    """
    Save meta-model coefficients to CSV file.
    
    Args:
        meta_model: Trained LogisticRegression meta-model
        base_model_names: List of base model names (feature names)
        output_dir: Output directory for the CSV file
    """
    try:
        if not hasattr(meta_model, 'coef_'):
            logger.warning("Meta-model does not have coefficients (not a linear model)")
            return
        
        # Extract coefficients
        coefficients = meta_model.coef_[0] if meta_model.coef_.ndim > 1 else meta_model.coef_
        intercept = meta_model.intercept_[0] if hasattr(meta_model, 'intercept_') else 0.0
        
        # Prepare data for CSV
        coef_data = []
        for i, model_name in enumerate(base_model_names):
            coef_data.append({
                'feature': model_name,
                'coefficient': coefficients[i] if i < len(coefficients) else 0.0
            })
        
        # Add intercept
        coef_data.append({
            'feature': 'intercept',
            'coefficient': intercept
        })
        
        # Create DataFrame and save to CSV
        import pandas as pd
        df_coef = pd.DataFrame(coef_data)
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        coef_path = os.path.join(output_dir, 'meta_model_coefficients.csv')
        df_coef.to_csv(coef_path, index=False)
        logger.info(f"ðŸ§  Meta-model coefficients saved to: {coef_path}")
        
    except Exception as e:
        logger.error(f"âŒ Error saving meta-model coefficients: {e}")


def prepare_calibration_method(method: str):
    """
    Prepare calibration method for probability calibration.
    
    Args:
        method: Calibration method ('isotonic', 'sigmoid', or 'none')
        
    Returns:
        Calibration method string or None
    """
    if method in ('isotonic', 'sigmoid'):
        return method
    return None


def _determine_output_directory(batch_num: Any = None, base_dir: str = "reports") -> tuple[str, str]:
    """Helper function to determine output directory and suffix for visualizations."""
    if batch_num is not None:
        if isinstance(batch_num, str) and '_' in batch_num:
            batch_parts = batch_num.split('_')
            if len(batch_parts) > 1:
                batch_dir = f"{base_dir}/batch multiple"
                suffix = f"_batch_multiple"
            else:
                batch_dir = f"{base_dir}/batch {batch_num}"
                suffix = f"_batch_{batch_num}"
        else:
            batch_dir = f"{base_dir}/batch {batch_num}"
            suffix = f"_batch_{batch_num}"
    else:
        batch_dir = base_dir
        suffix = ""
    return batch_dir, suffix


def _setup_output_directory(batch_dir: str) -> None:
    """Helper function to setup output directory for visualizations."""
    import shutil
    
    logger.info(f"ðŸ“Š Creating visualizations in directory: {batch_dir}")
    
    # Recreate directory
    if os.path.exists(batch_dir):
        shutil.rmtree(batch_dir)
        logger.info(f"ðŸ—‘ï¸  Cleaned existing directory: {batch_dir}")
    
    os.makedirs(batch_dir, exist_ok=True)
    logger.info(f"ðŸ“ Created visualization directory: {batch_dir}")


def _create_metric_bar_chart(metric_name: str, metrics: Dict[str, Dict[str, float]], 
                           batch_dir: str, suffix: str, title_prefix: str = "Committee") -> bool:
    """Helper function to create metric comparison bar charts."""
    try:
        import matplotlib.pyplot as plt
        
        logger.info(f"ðŸ“Š Generating {metric_name} comparison chart...")
        
        plt.figure(figsize=(CHART_FIGURE_WIDTH, CHART_FIGURE_HEIGHT))
        
        # Extract model names and values (excluding meta_model for base comparison)
        model_names = [name for name in metrics.keys() if name != 'meta_model']
        values = [metrics[name][metric_name] for name in model_names]
        
        # Add meta-model if it exists
        if 'meta_model' in metrics:
            model_names.append('meta_model')
            values.append(metrics['meta_model'][metric_name])
        
        # Create bar plot with different colors for meta-model
        colors = ['skyblue'] * (len(model_names) - 1)
        if 'meta_model' in model_names[-1:]:
            colors.append('orange')
        else:
            colors = ['skyblue'] * len(model_names)
        
        bars = plt.bar(range(len(model_names)), values, color=colors)
        
        plt.ylim(0, 1)
        plt.title(f"{title_prefix} - {metric_name.upper()} Comparison")
        plt.ylabel(metric_name.upper())
        plt.xlabel('Model')
        plt.xticks(range(len(model_names)), [name.replace('_', ' ').title() for name in model_names], rotation=45)
        
        # Add value labels on bars
        for i, (bar, val) in enumerate(zip(bars, values)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f"{val:.3f}", ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = f"{batch_dir}/{metric_name}_comparison{suffix}.png"
        plt.savefig(plot_path, dpi=CHART_DPI, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ… Saved metric chart: {plot_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error generating {metric_name} chart: {e}")
        return False


def _create_confusion_matrix_plot(model_name: str, y_true, y_pred, batch_dir: str, suffix: str) -> bool:
    """Helper function to create confusion matrix plots."""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import confusion_matrix
        
        logger.info(f"ðŸ“Š Generating confusion matrix for {model_name}...")
        
        # Create confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(MATRIX_FIGURE_WIDTH, MATRIX_FIGURE_HEIGHT))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['No Trade', 'Trade'], 
                   yticklabels=['No Trade', 'Trade'])
        plt.title(f'Confusion Matrix - {model_name.replace("_", " ").title()}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        # Save the plot
        matrix_path = f"{batch_dir}/confusion_matrix_{model_name}{suffix}.png"
        plt.savefig(matrix_path, dpi=CHART_DPI, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ… Saved confusion matrix: {matrix_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error generating confusion matrix for {model_name}: {e}")
        return False


def _create_confusion_matrices_oof(models: Dict[str, Any], X_test: pd.DataFrame, y_test: pd.Series, 
                                   thresholds: Dict[str, float], output_dir: str, suffix: str = "") -> int:
    """
    Create confusion matrices for OOF models using averaged test predictions.
    
    Args:
        models: Dictionary containing model information including test predictions
        X_test: Test features 
        y_test: Test labels
        thresholds: Dictionary of optimal thresholds per model
        output_dir: Output directory for plots
        suffix: Filename suffix
        
    Returns:
        Number of confusion matrices created
    """
    from sklearn.metrics import confusion_matrix
    count = 0
    
    # Extract test predictions and create confusion matrices
    test_predictions = models.get('test_predictions', {})
    
    for model_name, test_proba in test_predictions.items():
        try:
            if len(test_proba) == 0:
                continue
                
            threshold = thresholds.get(model_name, 0.5)
            y_pred = (test_proba >= threshold).astype(int)
            
            # Create confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            
            plt.figure(figsize=(MATRIX_FIGURE_WIDTH, MATRIX_FIGURE_HEIGHT))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['No Trade', 'Trade'], 
                       yticklabels=['No Trade', 'Trade'])
            plt.title(f'Confusion Matrix - {model_name.replace("_", " ").title()}\n(Threshold: {threshold:.3f})')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            
            # Save the plot
            matrix_path = os.path.join(output_dir, f"confusion_matrix_{model_name}{suffix}.png")
            plt.savefig(matrix_path, dpi=CHART_DPI, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… Saved confusion matrix: {matrix_path}")
            count += 1
            
        except Exception as e:
            logger.error(f"âŒ Error creating confusion matrix for {model_name}: {e}")
            continue
    
    # Create confusion matrix for meta-model if available
    if 'meta_predictions' in models:
        try:
            y_pred_meta = models['meta_predictions']
            cm = confusion_matrix(y_test, y_pred_meta)
            
            plt.figure(figsize=(MATRIX_FIGURE_WIDTH, MATRIX_FIGURE_HEIGHT))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['No Trade', 'Trade'], 
                       yticklabels=['No Trade', 'Trade'])
            plt.title(f'Confusion Matrix - Meta Model')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            
            # Save the plot
            matrix_path = os.path.join(output_dir, f"confusion_matrix_meta_model{suffix}.png")
            plt.savefig(matrix_path, dpi=CHART_DPI, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… Saved meta-model confusion matrix: {matrix_path}")
            count += 1
            
        except Exception as e:
            logger.error(f"âŒ Error creating meta-model confusion matrix: {e}")
    
    return count


def _create_metric_comparison_charts(metrics: Dict[str, Dict[str, float]], output_dir: str, 
                                   suffix: str = "", title_prefix: str = "Committee") -> int:
    """
    Create bar charts comparing F1, ROC AUC, and PR AUC across models.
    
    Args:
        metrics: Dictionary of metrics for each model
        output_dir: Output directory for plots
        suffix: Filename suffix
        title_prefix: Prefix for chart titles
        
    Returns:
        Number of charts created
    """
    metric_names = ['f1', 'roc_auc', 'pr_auc']
    metric_display_names = {'f1': 'F1 Score', 'roc_auc': 'ROC AUC', 'pr_auc': 'PR AUC'}
    count = 0
    
    for metric_name in metric_names:
        try:
            logger.info(f"ðŸ“Š Generating {metric_display_names[metric_name]} comparison chart...")
            
            plt.figure(figsize=(CHART_FIGURE_WIDTH, CHART_FIGURE_HEIGHT))
            
            # Extract model names and values (separate base models from meta-model)
            base_models = [name for name in metrics.keys() if name != 'meta_model']
            base_values = [metrics[name][metric_name] for name in base_models]
            
            # Prepare colors
            colors = ['skyblue'] * len(base_models)
            model_names = base_models[:]
            values = base_values[:]
            
            # Add meta-model if it exists
            if 'meta_model' in metrics:
                model_names.append('meta_model')
                values.append(metrics['meta_model'][metric_name])
                colors.append('orange')
            
            # Create bar plot
            bars = plt.bar(range(len(model_names)), values, color=colors)
            
            plt.ylim(0, 1)
            plt.title(f"{title_prefix} - {metric_display_names[metric_name]} Comparison")
            plt.ylabel(metric_display_names[metric_name])
            plt.xlabel('Model')
            plt.xticks(range(len(model_names)), [name.replace('_', ' ').title() for name in model_names], rotation=45)
            
            # Add value labels on bars
            for i, (bar, val) in enumerate(zip(bars, values)):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                        f"{val:.3f}", ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.join(output_dir, f"{metric_name}_comparison{suffix}.png")
            plt.savefig(plot_path, dpi=CHART_DPI, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… Saved {metric_display_names[metric_name]} chart: {plot_path}")
            count += 1
            
        except Exception as e:
            logger.error(f"âŒ Error generating {metric_display_names[metric_name]} chart: {e}")
            continue
    
    return count


def create_visualizations_unified(models: Dict[str, Any], X_test: pd.DataFrame, y_test: pd.Series, 
                                 metrics: Dict[str, Dict[str, float]], batch_num: Any = None,
                                 mode: str = "oof", output_dir: str = "reports") -> None:
    """
    Unified visualization function for both OOF and advanced training modes.
    
    Creates confusion matrices and metric comparison charts (F1, ROC AUC, PR AUC).
    All plots are saved with descriptive names in the specified output directory.
    
    Args:
        models: Dictionary containing trained models and predictions
        X_test: Test features
        y_test: Test labels  
        metrics: Nested dictionary of metrics for each model
        batch_num: Optional batch identifier used to name the output directory
        mode: Training mode ("oof" for out-of-fold, "advanced" for advanced training)
        output_dir: Base output directory for saving plots
    """
    if not VISUALIZATION_AVAILABLE:
        logger.warning("Visualization libraries not available; skipping plots.")
        return
    
    try:
        # Setup output directory
        batch_dir, suffix = _determine_output_directory(batch_num, output_dir)
        _setup_output_directory(batch_dir)
        
        title_prefix = "Out-of-Fold Committee" if mode == "oof" else "Advanced Committee"
        
        # Generate confusion matrices
        confusion_matrix_count = 0
        if mode == "oof":
            # For OOF mode, use test predictions from the models dictionary
            thresholds = models.get('thresholds', {})
            confusion_matrix_count = _create_confusion_matrices_oof(models, X_test, y_test, thresholds, batch_dir, suffix)
        else:
            # For advanced mode, use direct model predictions
            for model_name in models.keys():
                if model_name not in ['meta_model', 'thresholds', 'oof_predictions', 'test_predictions', 'meta_test_features', 'meta_predictions']:
                    try:
                        model = models[model_name]
                        if hasattr(model, 'predict'):
                            y_pred = model.predict(X_test)
                            if _create_confusion_matrix_plot(model_name, y_test, y_pred, batch_dir, suffix):
                                confusion_matrix_count += 1
                    except Exception as e:
                        logger.error(f"âŒ Error creating confusion matrix for {model_name}: {e}")
                        continue
        
        # Generate metric comparison charts (F1, ROC AUC, PR AUC)
        metric_chart_count = _create_metric_comparison_charts(metrics, batch_dir, suffix, title_prefix)
        
        # Export metrics to CSV
        thresholds = models.get('thresholds', {})
        export_metrics_to_csv(metrics, thresholds, batch_dir)
        
        # Save meta-model coefficients if available
        if 'meta_model' in models:
            meta_model = models['meta_model']
            base_model_names = [name for name in metrics.keys() if name != 'meta_model']
            save_meta_model_coefficients(meta_model, base_model_names, batch_dir)
        
        # Log summary
        total_images = confusion_matrix_count + metric_chart_count
        logger.info(f"ðŸ“Š Generated {confusion_matrix_count} confusion matrices and {metric_chart_count} metric comparison charts")
        logger.info(f"ðŸŽ‰ Visualization creation complete! Total images: {total_images}")
        logger.info(f"ðŸ“‚ All visualizations saved to: {batch_dir}")
        
    except Exception as e:
        logger.error(f"âŒ Critical error in visualization creation: {e}")
        return


def create_visualizations_oof(models: Dict[str, Any], X_test: pd.DataFrame, y_test: pd.Series, 
                             metrics: Dict[str, Dict[str, float]], batch_num: Any = None, output_dir: str = "reports") -> None:
    """
    Generate and save confusion matrices and metric bar charts for out-of-fold stacking models.
    
    This function now delegates to the unified visualization function.
    
    Args:
        models: Dictionary containing trained models and predictions from OOF training
        X_test: Test features
        y_test: Test labels
        metrics: Nested dictionary of metrics for each model
        batch_num: Optional batch identifier used to name the output directory
        output_dir: Base output directory for saving plots
    """
    create_visualizations_unified(models, X_test, y_test, metrics, batch_num, mode="oof", output_dir=output_dir)


def create_visualizations_advanced(models: Dict[str, Any], X_test: pd.DataFrame, y_test: pd.Series, 
                                  metrics: Dict[str, Dict[str, float]], batch_num: Any = None, output_dir: str = "reports") -> None:
    """
    Generate and save confusion matrices and metric bar charts for advanced models.

    This function now delegates to the unified visualization function.

    Args:
        models: Dictionary containing trained models and predictions
        X_test: Test features
        y_test: Test labels
        metrics: Nested dictionary of metrics for each model
        batch_num: Optional batch identifier used to name the output directory
        output_dir: Base output directory for saving plots
    """
    create_visualizations_unified(models, X_test, y_test, metrics, batch_num, mode="advanced", output_dir=output_dir)
    if not VISUALIZATION_AVAILABLE:
        logger.warning("Visualization libraries not available; skipping plots.")
        return
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import confusion_matrix
        
        # Determine output directory
        if batch_num is not None:
            if isinstance(batch_num, str) and '_' in batch_num:
                batch_parts = batch_num.split('_')
                if len(batch_parts) > 1:
                    batch_dir = f"reports/batch multiple"
                    suffix = f"_batch_multiple"
                else:
                    batch_dir = f"reports/batch {batch_num}"
                    suffix = f"_batch_{batch_num}"
            else:
                batch_dir = f"reports/batch {batch_num}"
                suffix = f"_batch_{batch_num}"
        else:
            batch_dir = "reports"
            suffix = ""
        
        logger.info(f"ðŸ“Š Creating advanced visualizations in directory: {batch_dir}")
        
        # Recreate directory
        if os.path.exists(batch_dir):
            import shutil
            shutil.rmtree(batch_dir)
            logger.info(f"ðŸ—‘ï¸  Cleaned existing directory: {batch_dir}")
        
        os.makedirs(batch_dir, exist_ok=True)
        logger.info(f"ðŸ“ Created visualization directory: {batch_dir}")
        
        # Generate confusion matrices
        confusion_matrix_count = 0
        for name, model_info in models.items():
            try:
                logger.info(f"ðŸ“ˆ Generating confusion matrix for {name}...")
                
                if name == 'stacked':
                    # Handle stacked model predictions
                    test_meta_features = model_info['test_meta_features']
                    meta_model = model_info['meta_model']
                    optimal_threshold = model_info['optimal_threshold']
                    
                    pred_proba = meta_model.predict_proba(test_meta_features)[:, 1]
                    y_pred = (pred_proba >= optimal_threshold).astype(int)
                else:
                    # Handle individual model predictions with optimal threshold
                    optimal_threshold = model_info['optimal_threshold']
                    trained_models = model_info['trained_models']
                    
                    # Average predictions across folds
                    fold_predictions = []
                    for fold_model in trained_models:
                        fold_proba = fold_model.predict_proba(X_test)
                        fold_proba_1 = fold_proba[:, -1] if fold_proba.ndim > 1 else fold_proba
                        fold_predictions.append(fold_proba_1)
                    
                    avg_proba = np.mean(fold_predictions, axis=0)
                    y_pred = (avg_proba >= optimal_threshold).astype(int)
                
                # Create confusion matrix
                cm = confusion_matrix(y_test, y_pred)
                plt.figure(figsize=(4, 3))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
                plt.title(f"{name.replace('_', ' ').title()} Confusion Matrix\n(Threshold: {optimal_threshold:.3f})")
                plt.xlabel('Predicted')
                plt.ylabel('Actual')
                plt.tight_layout()
                
                # Save the plot
                plot_path = f"{batch_dir}/{name}_confusion_matrix{suffix}.png"
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                confusion_matrix_count += 1
                logger.info(f"âœ… Saved confusion matrix: {plot_path}")
                
            except Exception as e:
                logger.error(f"âŒ Error generating confusion matrix for {name}: {e}")
                continue
        
        logger.info(f"ðŸ“Š Generated {confusion_matrix_count} confusion matrices")
        
        # Bar charts for metrics (including threshold info)
        metric_names = ['accuracy', 'f1', 'roc_auc']
        metric_chart_count = 0
        
        for metric_name in metric_names:
            try:
                logger.info(f"ðŸ“Š Generating {metric_name} comparison chart...")
                
                plt.figure(figsize=(8, 5))
                labels = list(metrics.keys())
                values = [metrics[m][metric_name] for m in labels]
                thresholds = [models[m]['optimal_threshold'] for m in labels]
                
                # Create bar plot with threshold annotations
                bars = sns.barplot(x=labels, y=values, palette="viridis")
                plt.ylim(0, 1)
                plt.title(f"Advanced Committee - {metric_name.upper()} Comparison")
                plt.ylabel(metric_name.upper())
                plt.xlabel('Model')
                
                # Add value and threshold labels on bars
                for idx, (val, thresh) in enumerate(zip(values, thresholds)):
                    plt.text(idx, val + 0.02, f"{val:.3f}\n(T:{thresh:.3f})", 
                            ha='center', va='bottom', fontsize=8)
                
                # Rotate x-axis labels if they're long
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                # Save the plot
                plot_path = f"{batch_dir}/{metric_name}_comparison_advanced{suffix}.png"
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                metric_chart_count += 1
                logger.info(f"âœ… Saved metric chart: {plot_path}")
                
            except Exception as e:
                logger.error(f"âŒ Error generating {metric_name} chart: {e}")
                continue
        
        # Create threshold optimization plot
        try:
            logger.info("ðŸ“Š Generating threshold optimization summary...")
            
            plt.figure(figsize=(10, 6))
            model_names = list(models.keys())
            thresholds = [models[m]['optimal_threshold'] for m in model_names]
            f1_scores = [metrics[m]['f1'] for m in model_names]
            
            # Create subplot for thresholds and F1 scores
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # Threshold plot
            bars1 = ax1.bar(model_names, thresholds, color='skyblue')
            ax1.set_title('Optimal Thresholds by Model')
            ax1.set_ylabel('Threshold')
            ax1.set_ylim(0, 1)
            ax1.tick_params(axis='x', rotation=45)
            
            for bar, thresh in zip(bars1, thresholds):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{thresh:.3f}', ha='center', va='bottom')
            
            # F1 score plot
            bars2 = ax2.bar(model_names, f1_scores, color='lightgreen')
            ax2.set_title('F1 Scores with Optimal Thresholds')
            ax2.set_ylabel('F1 Score')
            ax2.set_ylim(0, max(f1_scores) * 1.2 if max(f1_scores) > 0 else 1)
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, f1 in zip(bars2, f1_scores):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                        f'{f1:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plot_path = f"{batch_dir}/threshold_optimization_summary{suffix}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… Saved threshold optimization summary: {plot_path}")
            metric_chart_count += 1
            
        except Exception as e:
            logger.error(f"âŒ Error generating threshold plot: {e}")
        
        logger.info(f"ðŸ“Š Generated {metric_chart_count} metric comparison charts")
        logger.info(f"ðŸŽ‰ Advanced visualization creation complete! Total images: {confusion_matrix_count + metric_chart_count}")
        logger.info(f"ðŸ“‚ All visualizations saved to: {batch_dir}")
        
    except Exception as e:
        logger.error(f"âŒ Critical error in advanced visualization creation: {e}")
        return
    """
    Generate and save confusion matrices and metric bar charts for each model.

    Each model's confusion matrix is saved into the designated batch
    directory.  In addition, three bar charts (accuracy, F1, ROC-AUC) are
    produced across all models for easy comparison.

    Args:
        models: Mapping of model names to model instances (including the metaâ€‘model)
        X_test: Test features
        y_test: Test labels
        metrics: Nested dictionary of metrics for each model
        batch_num: Optional batch identifier used to name the output directory
    """
    if not VISUALIZATION_AVAILABLE:
        logger.warning("Visualization libraries not available; skipping plots.")
        return
    
    try:
        import matplotlib.pyplot as plt  # reimport within function to appease linters
        import seaborn as sns
        from sklearn.metrics import confusion_matrix
        
        # Determine output directory
        if batch_num is not None:
            # Create user-friendly batch directory names
            if isinstance(batch_num, str) and '_' in batch_num:
                # Handle multiple batches like "1_2" -> "batch multiple"
                batch_parts = batch_num.split('_')
                if len(batch_parts) > 1:
                    batch_dir = f"reports/batch multiple"
                    suffix = f"_batch_multiple"
                else:
                    batch_dir = f"reports/batch {batch_num}"
                    suffix = f"_batch_{batch_num}"
            else:
                # Handle single batch like "1" -> "batch 1"
                batch_dir = f"reports/batch {batch_num}"
                suffix = f"_batch_{batch_num}"
        else:
            batch_dir = "reports"
            suffix = ""
        
        logger.info(f"ðŸ“Š Creating visualizations in directory: {batch_dir}")
        
        # Recreate directory
        if os.path.exists(batch_dir):
            import shutil
            shutil.rmtree(batch_dir)
            logger.info(f"ðŸ—‘ï¸  Cleaned existing directory: {batch_dir}")
        
        os.makedirs(batch_dir, exist_ok=True)
        logger.info(f"ðŸ“ Created visualization directory: {batch_dir}")
        
        # Compute confusion matrices and plot
        confusion_matrix_count = 0
        for name, model in models.items():
            try:
                logger.info(f"ðŸ“ˆ Generating confusion matrix for {name}...")
                
                if name == 'stacked':
                    # Recompute stacked probabilities using base models
                    # Create stacked features again from base models
                    base_names = ['xgboost', 'lightgbm', 'catboost', 'random_forest', 'svc']
                    stacked = []
                    for b in base_names:
                        if b in models:
                            proba = models[b].predict_proba(X_test)
                            stacked.append(proba[:, -1] if proba.ndim > 1 else proba)
                        else:
                            logger.warning(f"Model {b} not found for stacking")
                    
                    if stacked:
                        stacked_arr = np.column_stack(stacked)
                        pred_proba = model.predict_proba(stacked_arr)
                        y_pred = (pred_proba[:, -1] >= 0.001).astype(int)  # Extremely low threshold
                    else:
                        logger.error(f"No base models available for stacking {name}")
                        continue
                else:
                    pred_proba = model.predict_proba(X_test)
                    y_pred = (pred_proba[:, -1] >= 0.001).astype(int)  # Extremely low threshold
                
                # Create confusion matrix
                cm = confusion_matrix(y_test, y_pred)
                plt.figure(figsize=(4, 3))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
                plt.title(f"{name.replace('_', ' ').title()} Confusion Matrix")
                plt.xlabel('Predicted')
                plt.ylabel('Actual')
                plt.tight_layout()
                
                # Save the plot
                plot_path = f"{batch_dir}/{name}_confusion_matrix{suffix}.png"
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                confusion_matrix_count += 1
                logger.info(f"âœ… Saved confusion matrix: {plot_path}")
                
            except Exception as e:
                logger.error(f"âŒ Error generating confusion matrix for {name}: {e}")
                continue
        
        logger.info(f"ðŸ“Š Generated {confusion_matrix_count} confusion matrices")
        
        # Bar charts for metrics
        metric_names = ['accuracy', 'f1', 'roc_auc']
        metric_chart_count = 0
        
        for metric_name in metric_names:
            try:
                logger.info(f"ðŸ“Š Generating {metric_name} comparison chart...")
                
                plt.figure(figsize=(6, 4))
                labels = list(metrics.keys())
                values = [metrics[m][metric_name] for m in labels]
                
                # Create bar plot
                bars = sns.barplot(x=labels, y=values, palette="viridis")
                plt.ylim(0, 1)
                plt.title(f"Model {metric_name.upper()} Comparison")
                plt.ylabel(metric_name.upper())
                plt.xlabel('Model')
                
                # Add value labels on bars
                for idx, val in enumerate(values):
                    plt.text(idx, val + 0.02, f"{val:.3f}", ha='center', va='bottom')
                
                # Rotate x-axis labels if they're long
                plt.xticks(rotation=45)
                plt.tight_layout()
                
                # Save the plot
                plot_path = f"{batch_dir}/{metric_name}_comparison{suffix}.png"
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                metric_chart_count += 1
                logger.info(f"âœ… Saved metric chart: {plot_path}")
                
            except Exception as e:
                logger.error(f"âŒ Error generating {metric_name} chart: {e}")
                continue
        
        logger.info(f"ðŸ“Š Generated {metric_chart_count} metric comparison charts")
        logger.info(f"ðŸŽ‰ Visualization creation complete! Total images: {confusion_matrix_count + metric_chart_count}")
        logger.info(f"ðŸ“‚ All visualizations saved to: {batch_dir}")
        
    except Exception as e:
        logger.error(f"âŒ Critical error in visualization creation: {e}")
        return


def collect_alpaca_training_data(batch_numbers: List[int], max_symbols_per_batch: int = 10, use_cached: bool = True) -> pd.DataFrame:
    """
    Collect training data from Alpaca API for specified batches.
    
    Args:
        batch_numbers: List of batch numbers to process
        max_symbols_per_batch: Maximum symbols to process per batch
        use_cached: Whether to use cached data if available
        
    Returns:
        DataFrame with engineered features and targets ready for training
    """
    cache_file = f"alpaca_training_data_batches_{'_'.join(map(str, batch_numbers))}.csv"
    
    # Check for cached data
    if use_cached and os.path.exists(cache_file):
        logger.info(f"Loading cached training data from {cache_file}")
        try:
            df = pd.read_csv(cache_file)
            logger.info(f"âœ… Loaded {len(df)} samples from cache")
            return df
        except Exception as e:
            logger.warning(f"Failed to load cached data: {e}, collecting fresh data")
    
    # Collect fresh data from Alpaca
    logger.info(f"Collecting fresh training data from Alpaca API...")
    try:
        collector = AlpacaDataCollector()
        df = collector.collect_training_data(
            batch_numbers=batch_numbers,
            max_symbols_per_batch=max_symbols_per_batch
        )
        
        if len(df) > 0:
            # Cache the data
            df.to_csv(cache_file, index=False)
            logger.info(f"ðŸ’¾ Training data cached to {cache_file}")
        
        return df
        
    except Exception as e:
        logger.error(f"Failed to collect Alpaca data: {e}")
        raise


def main(batch_numbers: List[int] = None, max_symbols_per_batch: int = 10, use_alpaca: bool = True, 
         stacking_method: str = 'oof', n_folds: int = DEFAULT_N_FOLDS, sampler: str = 'smoteenn',
         calibrate: str = DEFAULT_CALIBRATION_METHOD, threshold: float = 0.5, output_dir: str = 'reports') -> None:
    """
    Main function for Committee of Five training with Alpaca data integration.
    
    Args:
        batch_numbers: List of batch numbers to process (default: [1, 2])
        max_symbols_per_batch: Maximum symbols per batch (default: 10)
        use_alpaca: Whether to use Alpaca API for data collection (default: True)
        stacking_method: Stacking method 'oof' (out-of-fold) or 'standard' (default: 'oof')
        n_folds: Number of cross-validation folds (default: DEFAULT_N_FOLDS)
        sampler: Sampling method for class imbalance ('smoteenn' or 'none', default: 'smoteenn')
        calibrate: Calibration method ('isotonic', 'sigmoid', or 'none', default: 'isotonic')
        threshold: Default probability threshold for classification (default: 0.5)
        output_dir: Directory for saving plots (default: 'reports')
    """
    if batch_numbers is None:
        batch_numbers = [1, 2]  # Default to first 2 batches for testing
    
    logger.info(f"ðŸš€ Starting Committee of Five training for batches: {batch_numbers}")
    logger.info(f"ðŸ“Š Using {stacking_method.upper()} stacking method")
    logger.info(f"âš™ï¸  Configuration: n_folds={n_folds}, sampler={sampler}, calibrate={calibrate}, threshold={threshold}")
    
    try:
        if use_alpaca:
            # Collect data from Alpaca API
            df = collect_alpaca_training_data(
                batch_numbers=batch_numbers,
                max_symbols_per_batch=max_symbols_per_batch,
                use_cached=True
            )
        else:
            # Fallback to CSV file method
            data_path = os.environ.get('COMMITTEE_DATA_PATH', 'data.csv')
            if not os.path.exists(data_path):
                logger.error(f"Data file '{data_path}' not found. Set COMMITTEE_DATA_PATH to override.")
                return
            df = pd.read_csv(data_path)
        
        if len(df) == 0:
            logger.error("No training data available")
            return
        
        # Assume a 'target' column exists and drop rows without it
        if 'target' not in df.columns:
            logger.error("The training data must contain a 'target' column for labels.")
            return
        
        # Use all columns except the target and ticker as features
        feature_columns = [col for col in df.columns if col not in ['target', 'ticker']]
        logger.info(f"ðŸ“‹ Using {len(feature_columns)} features for training")
        
        # Prepare train/test split
        X_train, X_test, y_train, y_test = prepare_training_data(df, feature_columns, target_column='target')
        
        logger.info(f"ðŸ“Š Training set: {len(X_train)} samples")
        logger.info(f"ðŸ“Š Test set: {len(X_test)} samples")
        
        # Train committee models with specified stacking method
        if stacking_method == 'oof':
            models, metrics = train_committee_models_advanced(X_train, y_train, X_test, y_test)
        else:
            # Keep the original method for backward compatibility
            logger.warning("Standard stacking method not implemented in this version. Using OOF method.")
            models, metrics = train_committee_models_advanced(X_train, y_train, X_test, y_test)
        
        # Generate visualizations using the unified approach with configurable output directory
        if VISUALIZATION_AVAILABLE:
            batch_suffix = '_'.join(map(str, batch_numbers)) if batch_numbers else 'default'
            # Create visualizations in the specified output directory
            create_visualizations_unified(
                models, X_test, y_test, metrics, 
                batch_num=batch_suffix, mode="oof" if stacking_method == 'oof' else "advanced",
                output_dir=output_dir
            )
        
        # Log training summary
        log_training_summary(
            batch_number=batch_numbers[0] if batch_numbers else 0,
            symbols_trained=df['ticker'].nunique() if 'ticker' in df.columns else len(df),
            xgb_accuracy=metrics.get('xgboost', {}).get('accuracy', 0.0),
            nn_accuracy=metrics.get('stacked', {}).get('accuracy', 0.0),  # Use meta-model as "NN"
            training_time_seconds=0.0,  # not measured in this simplified example
            timeframe='Alpaca_API_Data' if use_alpaca else 'CSV_Data'
        )
        
        logger.info("ðŸŽ‰ Committee of Five training completed successfully!")
        
        # Print final metrics summary
        logger.info("\nðŸ“Š Final Model Performance Summary:")
        for model_name, model_metrics in metrics.items():
            accuracy = model_metrics.get('accuracy', 0)
            f1 = model_metrics.get('f1', 0)
            logger.info(f"   {model_name}: Accuracy={accuracy:.3f}, F1={f1:.3f}")
            
    except Exception as e:
        logger.error(f"Training failed: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Committee of Five Training with Alpaca Integration')
    parser.add_argument('--batches', nargs='+', type=int, default=[1, 2], 
                       help='Batch numbers to process (default: 1 2)')
    parser.add_argument('--max-symbols', type=int, default=10,
                       help='Maximum symbols per batch (default: 10)')
    parser.add_argument('--no-alpaca', action='store_true',
                       help='Use CSV file instead of Alpaca API')
    parser.add_argument('--force-fresh', action='store_true',
                       help='Force fresh data collection (ignore cache)')
    parser.add_argument('--stacking', choices=['oof', 'standard'], default='oof',
                       help='Stacking method: oof (out-of-fold) or standard (default: oof)')
    
    # Add centralized CLI flags for model configuration
    parser.add_argument('--n-folds', type=int, default=DEFAULT_N_FOLDS,
                       help=f'Number of CV folds (default: {DEFAULT_N_FOLDS})')
    parser.add_argument('--sampler', choices=['smoteenn', 'none'], default='smoteenn',
                       help='Sampling method to balance classes (default: smoteenn)')
    parser.add_argument('--calibrate', choices=['isotonic', 'sigmoid', 'none'], default=DEFAULT_CALIBRATION_METHOD,
                       help=f'Calibration method (default: {DEFAULT_CALIBRATION_METHOD})')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='Default probability threshold for classification (default: 0.5)')
    parser.add_argument('--output-dir', type=str, default='reports',
                       help='Directory for saving plots (default: reports)')
    
    args = parser.parse_args()
    
    # Override cache setting if force-fresh is specified
    if args.force_fresh:
        # Clear any existing cache files
        import glob
        cache_files = glob.glob("alpaca_training_data_batches_*.csv")
        for cache_file in cache_files:
            try:
                os.remove(cache_file)
                logger.info(f"ðŸ—‘ï¸  Removed cache file: {cache_file}")
            except Exception as e:
                logger.warning(f"Could not remove cache file {cache_file}: {e}")
    
    main(
        batch_numbers=args.batches,
        max_symbols_per_batch=args.max_symbols,
        use_alpaca=not args.no_alpaca,
        stacking_method=args.stacking,
        n_folds=args.n_folds,
        sampler=args.sampler,
        calibrate=args.calibrate,
        threshold=args.threshold,
        output_dir=args.output_dir
    )