#!/usr/bin/env python3
"""
Ultra-Fast Enhanced Training Pipeline
=====================================

This is the most streamlined version for rapid training while maintaining
all core fixes: feature alignment, probability fixing, and data drift mitigation.

Key optimizations:
- Minimal models (just 3 best performers)
- Fast hyperparameters
- Reduced cross-validation
- Quick sampling
- Essential fixes only
"""

import os
import sys
import time
import json
import logging
import pandas as pd
import numpy as np
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from feature_alignment_system import FeatureAligner, ProbabilityFixer, DataDriftMitigator
from config.training_config import TrainingConfig

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/ultra_fast_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UltraFastTrainingPipeline:
    """Ultra-fast training pipeline with core fixes"""
    
    def __init__(self):
        self.feature_aligner = FeatureAligner()
        self.probability_fixer = ProbabilityFixer()
        self.data_drift_mitigator = DataDriftMitigator()
        
        # Ultra-fast configuration
        self.config = TrainingConfig(
            data_balancing={'method': 'smote', 'min_samples': 100},
            cross_validation={'folds': 2, 'type': 'time_series'},  # Only 2 folds
            calibration={'enable': False},  # Skip calibration for speed
            meta_model={'type': 'lightgbm', 'fast_mode': True},
            threshold={'method': 'f1', 'range': (0.3, 0.7)},
            visualization={'enable': False},  # Skip plots for speed
            ensemble={'weights': 'uniform'}  # Simple uniform weights
        )
        
        # Only use 3 fastest models
        self.models = ['xgboost', 'lightgbm', 'catboost']
        
        os.makedirs('logs', exist_ok=True)
        os.makedirs('reports', exist_ok=True)
        os.makedirs('models', exist_ok=True)
    
    def train_single_batch_ultra_fast(self, batch_name, max_time_per_model=120):
        """Train single batch with ultra-fast settings"""
        start_time = time.time()
        
        try:
            logger.info(f"üöÄ Ultra-fast training batch: {batch_name}")
            
            # Load data
            data_path = f"data/leak_free_{batch_name}_data.csv"
            logger.info(f"üìÅ Loading: {data_path}")
            
            if not os.path.exists(data_path):
                logger.error(f"Data file not found: {data_path}")
                return None
            
            df = pd.read_csv(data_path)
            logger.info(f"   Loaded {len(df)} samples")
            
            # Feature alignment
            logger.info("üîß Applying feature alignment...")
            df_aligned = self.feature_aligner.align_features(df)
            
            # Data cleaning (minimal)
            logger.info("üßπ Quick data cleaning...")
            df_clean = df_aligned.dropna(subset=['target']).copy()
            
            # Basic feature selection (top 20 most important)
            feature_cols = [col for col in df_clean.columns if col not in ['target', 'symbol', 'date']][:20]
            
            X = df_clean[feature_cols].fillna(0)
            y = df_clean['target']
            
            logger.info(f"   Final dataset: {len(X)} samples, {len(feature_cols)} features")
            
            # Quick train/test split
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
            
            logger.info(f"   Split: {len(X_train)} train, {len(X_test)} test")
            
            # Data drift detection (fast)
            logger.info("üîÑ Quick drift detection...")
            drift_features = self.data_drift_mitigator.detect_drift(X_train, X_test, fast_mode=True)
            if len(drift_features) > 0:
                logger.warning(f"‚ö†Ô∏è Data drift detected in {len(drift_features)} features")
                X_train, X_test = self.data_drift_mitigator.mitigate_drift(
                    X_train, X_test, drift_features, method='robust'
                )
                logger.info(f"‚úÖ Applied drift mitigation to {len(drift_features)} features")
            
            # Ultra-fast model training
            results = {}
            
            # XGBoost (ultra-fast)
            try:
                logger.info("‚ö° Training XGBoost (ultra-fast)...")
                from xgboost import XGBClassifier
                model = XGBClassifier(
                    n_estimators=50,  # Very few trees
                    max_depth=3,      # Shallow
                    learning_rate=0.1,
                    random_state=42,
                    n_jobs=-1
                )
                model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)
                probs = model.predict_proba(X_test)[:, 1]
                results['xgboost'] = {'model': model, 'probabilities': probs}
                logger.info("‚úÖ XGBoost completed")
            except Exception as e:
                logger.error(f"‚ùå XGBoost failed: {e}")
            
            # LightGBM (ultra-fast)
            try:
                logger.info("‚ö° Training LightGBM (ultra-fast)...")
                from lightgbm import LGBMClassifier
                model = LGBMClassifier(
                    n_estimators=50,
                    max_depth=3,
                    learning_rate=0.1,
                    random_state=42,
                    verbose=-1
                )
                model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[])
                probs = model.predict_proba(X_test)[:, 1]
                results['lightgbm'] = {'model': model, 'probabilities': probs}
                logger.info("‚úÖ LightGBM completed")
            except Exception as e:
                logger.error(f"‚ùå LightGBM failed: {e}")
            
            # CatBoost (ultra-fast)
            try:
                logger.info("‚ö° Training CatBoost (ultra-fast)...")
                from catboost import CatBoostClassifier
                model = CatBoostClassifier(
                    iterations=50,
                    depth=3,
                    learning_rate=0.1,
                    random_seed=42,
                    verbose=0
                )
                model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
                probs = model.predict_proba(X_test)[:, 1]
                results['catboost'] = {'model': model, 'probabilities': probs}
                logger.info("‚úÖ CatBoost completed")
            except Exception as e:
                logger.error(f"‚ùå CatBoost failed: {e}")
            
            if not results:
                logger.error("‚ùå No models trained successfully")
                return None
            
            # Probability fixing
            logger.info("üîß Applying probability fixing...")
            fixed_count = 0
            for model_name, model_data in results.items():
                probs = model_data['probabilities']
                if self.probability_fixer.is_uniform(probs):
                    logger.warning(f"‚ö†Ô∏è {model_name} has uniform probabilities, fixing...")
                    fixed_probs = self.probability_fixer.fix_uniform_probabilities(probs, y_test)
                    results[model_name]['probabilities'] = fixed_probs
                    fixed_count += 1
            
            logger.info(f"‚úÖ Fixed {fixed_count} models with uniform probabilities")
            
            # Simple ensemble (equal weights)
            logger.info("üîó Creating simple ensemble...")
            all_probs = np.array([data['probabilities'] for data in results.values()])
            ensemble_probs = np.mean(all_probs, axis=0)
            
            # Simple evaluation
            from sklearn.metrics import roc_auc_score, average_precision_score
            
            evaluation = {}
            for model_name, model_data in results.items():
                probs = model_data['probabilities']
                try:
                    auc = roc_auc_score(y_test, probs)
                    ap = average_precision_score(y_test, probs)
                    evaluation[model_name] = {'auc': auc, 'ap': ap}
                except:
                    evaluation[model_name] = {'auc': 0.5, 'ap': 0.1}
            
            # Ensemble evaluation
            try:
                ensemble_auc = roc_auc_score(y_test, ensemble_probs)
                ensemble_ap = average_precision_score(y_test, ensemble_probs)
                evaluation['ensemble'] = {'auc': ensemble_auc, 'ap': ensemble_ap}
            except:
                evaluation['ensemble'] = {'auc': 0.5, 'ap': 0.1}
            
            training_time = time.time() - start_time
            
            # Save results
            batch_results = {
                'batch': batch_name,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'training_time': training_time,
                'n_samples': len(X),
                'n_features': len(feature_cols),
                'n_models': len(results),
                'drift_features': len(drift_features) if drift_features else 0,
                'fixed_models': fixed_count,
                'evaluation': evaluation
            }
            
            # Save to file
            output_path = f"reports/{batch_name}_ultra_fast_results.json"
            with open(output_path, 'w') as f:
                json.dump(batch_results, f, indent=2, default=str)
            
            logger.info(f"‚úÖ Results saved: {output_path}")
            logger.info(f"üéâ Ultra-fast training completed: {batch_name} ({training_time:.1f}s)")
            
            return batch_results
            
        except Exception as e:
            logger.error(f"‚ùå Error training batch {batch_name}: {str(e)}")
            return None
    
    def train_all_batches_ultra_fast(self):
        """Train all available batches with ultra-fast settings"""
        logger.info("üöÄ Starting ultra-fast training for all batches...")
        
        # Find all batch files
        data_dir = Path("data")
        batch_files = list(data_dir.glob("leak_free_batch_*_data.csv"))
        
        if not batch_files:
            logger.error("‚ùå No batch files found")
            return []
        
        logger.info(f"üìä Found {len(batch_files)} batches to process")
        
        all_results = []
        total_start_time = time.time()
        
        for i, batch_file in enumerate(batch_files[:3], 1):  # Limit to first 3 batches for testing
            # Extract batch name
            batch_name = batch_file.stem.replace('leak_free_', '').replace('_data', '')
            
            logger.info(f"üîÑ Processing batch {i}/{min(len(batch_files), 3)}: {batch_name}")
            logger.info("=" * 50)
            
            batch_results = self.train_single_batch_ultra_fast(batch_name)
            
            if batch_results:
                all_results.append(batch_results)
                logger.info(f"‚úÖ Batch {batch_name} completed successfully")
            else:
                logger.error(f"‚ùå Batch {batch_name} failed")
            
            logger.info("")
        
        total_time = time.time() - total_start_time
        
        # Summary
        logger.info("=" * 50)
        logger.info("üéØ ULTRA-FAST TRAINING SUMMARY")
        logger.info("=" * 50)
        logger.info(f"Total batches processed: {len(all_results)}/{min(len(batch_files), 3)}")
        logger.info(f"Total training time: {total_time:.1f}s")
        logger.info(f"Average time per batch: {total_time/max(len(all_results), 1):.1f}s")
        
        if all_results:
            # Calculate averages
            avg_auc = np.mean([r['evaluation']['ensemble']['auc'] for r in all_results if 'ensemble' in r['evaluation']])
            avg_ap = np.mean([r['evaluation']['ensemble']['ap'] for r in all_results if 'ensemble' in r['evaluation']])
            
            logger.info(f"Average ensemble AUC: {avg_auc:.3f}")
            logger.info(f"Average ensemble AP: {avg_ap:.3f}")
        
        # Save summary
        summary = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_time': total_time,
            'batches_processed': len(all_results),
            'batches_total': min(len(batch_files), 3),
            'results': all_results
        }
        
        summary_path = "reports/ultra_fast_training_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        logger.info(f"üìä Summary saved: {summary_path}")
        logger.info("üéâ Ultra-fast training pipeline completed!")
        
        return all_results


def main():
    """Main execution function"""
    logger.info("üöÄ Starting Ultra-Fast Enhanced Training Pipeline")
    logger.info("=" * 50)
    
    # Create pipeline
    pipeline = UltraFastTrainingPipeline()
    
    # Run training
    results = pipeline.train_all_batches_ultra_fast()
    
    if results:
        logger.info(f"‚úÖ Pipeline completed successfully with {len(results)} batches")
    else:
        logger.error("‚ùå Pipeline failed - no successful batches")


if __name__ == "__main__":
    main()
