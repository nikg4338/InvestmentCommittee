#!/usr/bin/env python3
"""
Validation Script for Enhanced Pipeline Components
=================================================

This script validates that all the key enhancement components are working correctly:
1. Ranking metrics integration
2. Multi-horizon target creation
3. SHAP feature selection
4. Dynamic weights computation
5. Time-series CV and LLM features configuration
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_ranking_metrics_integration():
    """Test that ranking metrics are properly integrated."""
    logger.info("üîç Testing ranking metrics integration...")
    
    try:
        from utils.evaluation import compute_enhanced_metrics
        from utils.ranking_metrics import compute_ranking_metrics, log_ranking_performance
        
        # Create synthetic test data
        np.random.seed(42)
        y_true = np.array([0] * 95 + [1] * 5)  # 5% positive class (extreme imbalance)
        y_proba = np.random.beta(1, 10, 100)  # Skewed probabilities
        y_pred = (y_proba > 0.1).astype(int)
        
        # Test enhanced metrics computation
        metrics = compute_enhanced_metrics(y_true, y_pred, y_proba, "TestModel")
        
        # Check that ranking metrics are included
        ranking_keys = ['precision_at_10', 'top_5pct_precision', 'mean_average_precision']
        found_keys = [key for key in ranking_keys if key in metrics]
        
        logger.info(f"   Found ranking metrics: {found_keys}")
        logger.info(f"   Total metrics computed: {len(metrics)}")
        
        if len(found_keys) >= 2:
            logger.info("   ‚úÖ PASS: Ranking metrics integration working")
            return True
        else:
            logger.error("   ‚ùå FAIL: Missing ranking metrics")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå FAIL: {e}")
        return False

def test_multi_horizon_targets():
    """Test multi-horizon target creation."""
    logger.info("üîç Testing multi-horizon target creation...")
    
    try:
        from data_collection_alpaca import AlpacaDataCollector
        
        # Create test data
        collector = AlpacaDataCollector()
        mock_df = pd.DataFrame({
            'close': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
            'open': [99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109],
            'high': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111],
            'low': [98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108],
            'volume': [1000000] * 11,
            'timestamp': pd.date_range('2024-01-01', periods=11)
        })
        
        # Test regression targets
        regression_targets = collector.create_target_variable(
            mock_df, 'TEST', use_regression=True, create_all_horizons=True
        )
        
        expected_reg_cols = ['target_1d_return', 'target_3d_return', 'target_5d_return', 'target_10d_return']
        reg_found = [col for col in expected_reg_cols if col in regression_targets.columns]
        
        # Test binary targets
        binary_targets = collector.create_target_variable(
            mock_df, 'TEST', use_regression=False, create_all_horizons=True
        )
        
        expected_bin_cols = ['target_1d_binary', 'target_3d_binary', 'target_5d_binary', 'target_10d_binary']
        bin_found = [col for col in expected_bin_cols if col in binary_targets.columns]
        
        logger.info(f"   Regression targets found: {reg_found}")
        logger.info(f"   Binary targets found: {bin_found}")
        
        if len(reg_found) >= 3 and len(bin_found) >= 3:
            logger.info("   ‚úÖ PASS: Multi-horizon target creation working")
            return True
        else:
            logger.error("   ‚ùå FAIL: Missing target horizons")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå FAIL: {e}")
        return False

def test_shap_feature_selection():
    """Test SHAP feature selection functionality."""
    logger.info("üîç Testing SHAP feature selection...")
    
    try:
        from utils.pipeline_improvements import select_top_features_shap
        from sklearn.ensemble import RandomForestClassifier
        
        # Create synthetic feature data
        np.random.seed(42)
        n_samples, n_features = 100, 15
        X = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        y = np.random.binomial(1, 0.1, n_samples)  # 10% positive class
        
        # Train a simple model
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X, y)
        
        # Test SHAP feature selection
        k = 8
        selected_features = select_top_features_shap(model, X, k=k)
        
        logger.info(f"   Original features: {n_features}")
        logger.info(f"   Requested features: {k}")
        logger.info(f"   Selected features: {len(selected_features)}")
        logger.info(f"   Top 5 selected: {selected_features[:5]}")
        
        # If SHAP is not available, the function should return all features and log a warning
        # This is acceptable behavior - the system should work with or without SHAP
        if len(selected_features) == k:
            logger.info("   ‚úÖ PASS: SHAP feature selection working properly")
            return True
        elif len(selected_features) == n_features:
            logger.info("   ‚úÖ PASS: SHAP not available, gracefully falling back to all features")
            return True
        else:
            logger.error("   ‚ùå FAIL: Unexpected feature selection behavior")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå FAIL: {e}")
        return False

def test_dynamic_weights():
    """Test dynamic weights computation."""
    logger.info("üîç Testing dynamic weights computation...")
    
    try:
        from utils.pipeline_improvements import compute_dynamic_weights
        
        # Create mock evaluation results
        evaluation_results = {
            'xgboost': {'roc_auc': 0.75, 'f1': 0.30},
            'lightgbm': {'roc_auc': 0.72, 'f1': 0.25},
            'catboost': {'roc_auc': 0.68, 'f1': 0.20},
            'random_forest': {'roc_auc': 0.65, 'f1': 0.18},
            'svm': {'roc_auc': 0.60, 'f1': 0.15}
        }
        
        # Test dynamic weights
        weights = compute_dynamic_weights(evaluation_results, metric='roc_auc')
        
        logger.info(f"   Models: {list(weights.keys())}")
        logger.info(f"   Weights sum: {sum(weights.values()):.6f}")
        logger.info(f"   Highest weight: {max(weights.values()):.4f}")
        logger.info(f"   Weight distribution: {weights}")
        
        # Validate weights
        if (len(weights) == 5 and 
            abs(sum(weights.values()) - 1.0) < 1e-6 and
            max(weights.values()) > min(weights.values())):
            logger.info("   ‚úÖ PASS: Dynamic weights computation working")
            return True
        else:
            logger.error("   ‚ùå FAIL: Dynamic weights computation issues")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå FAIL: {e}")
        return False

def test_configuration_settings():
    """Test that configuration settings are properly enabled."""
    logger.info("üîç Testing configuration settings...")
    
    try:
        from config.training_config import get_default_config
        
        config = get_default_config()
        
        # Check key settings
        settings_to_check = {
            'use_time_series_cv': True,
            'enable_llm_features': True,
            'enable_feature_selection': True,
            'use_xgb_meta_model': True,
            'enable_rolling_backtest': True,
            'enable_drift_detection': True
        }
        
        results = {}
        for setting, expected in settings_to_check.items():
            actual = getattr(config, setting, None)
            results[setting] = (actual, expected, actual == expected)
            
        logger.info("   Configuration validation:")
        for setting, (actual, expected, matches) in results.items():
            status = "‚úÖ" if matches else "‚ùå"
            logger.info(f"   {status} {setting}: {actual} (expected: {expected})")
        
        all_correct = all(matches for _, _, matches in results.values())
        
        if all_correct:
            logger.info("   ‚úÖ PASS: Configuration settings properly enabled")
            return True
        else:
            logger.error("   ‚ùå FAIL: Some configuration settings incorrect")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå FAIL: {e}")
        return False

def main():
    """Run all validation tests."""
    logger.info("üöÄ Validating enhanced pipeline components...")
    
    tests = [
        ("Ranking Metrics Integration", test_ranking_metrics_integration),
        ("Multi-Horizon Targets", test_multi_horizon_targets),
        ("SHAP Feature Selection", test_shap_feature_selection),
        ("Dynamic Weights", test_dynamic_weights),
        ("Configuration Settings", test_configuration_settings)
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\nüìã Running: {test_name}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            logger.error(f"‚ùå Test failed with exception: {e}")
            results.append((test_name, False))
    
    # Summary
    logger.info(f"\nüìä Validation Summary:")
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        logger.info(f"   {status}: {test_name}")
    
    logger.info(f"\nüéØ Overall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All enhancements validated successfully!")
        return True
    else:
        logger.error(f"‚ö†Ô∏è  {total - passed} tests failed - check implementations")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
